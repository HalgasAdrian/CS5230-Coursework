{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZAEq+PDHMuR0tAp3JDXRk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1: tSNE dim reduction**\n",
        "\n",
        "part A) Run tSNE library/package on MNIST and 20NG datasets, to obtain a representation is 2-dim or 3-dim, and visualize the data by plotting datapoints with a color per label. Try different values for perplexity like 5, 20 and 100."
      ],
      "metadata": {
        "id": "kT4D9kxK6uUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.datasets import fetch_20newsgroups, fetch_openml\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "8G2b18jI7s5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST Dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X_mnist = mnist.data[:2000]\n",
        "y_mnist = mnist.target[:2000].astype(int)\n",
        "\n",
        "# 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='train')\n",
        "vectorizer = CountVectorizer(max_features=1000)\n",
        "X_20ng = vectorizer.fit_transform(newsgroups.data[:2000]).toarray()\n",
        "y_20ng = newsgroups.target[:2000]\n",
        "\n",
        "# t-SNE visualization function\n",
        "def plot_tsne(X, y, dataset_name, perplexities=[5,20,100]):\n",
        "    for perp in perplexities:\n",
        "        tsne = TSNE(n_components=2, perplexity=perp, random_state=42)\n",
        "        X_embedded = tsne.fit_transform(X)\n",
        "        plt.figure(figsize=(8,6))\n",
        "        plt.scatter(X_embedded[:,0], X_embedded[:,1], c=y, cmap='tab20', s=5)\n",
        "        plt.colorbar()\n",
        "        plt.title(f\"{dataset_name} t-SNE with Perplexity={perp}\")\n",
        "        plt.show()\n",
        "\n",
        "# Plotting tSNE visualizations\n",
        "plot_tsne(X_mnist, y_mnist, 'MNIST')\n",
        "plot_tsne(X_20ng, y_20ng, '20 Newsgroups')"
      ],
      "metadata": {
        "id": "mxrCD3ld73TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "part B) Run DBscan on tSNE-MNIST of G=2,3,5 dimensions. This should work much better than DBscan on original MNIST or on PCA-MNIST. You should be able to observe most datapoints \"colored\" and the colors roughly corresponding to image labels. We got the following confusion matrix with tSNE into 3 dimensions, and some trial-and-error with DBSCAN parameters:"
      ],
      "metadata": {
        "id": "yz3k6S0B76gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dbscan_on_tsne(X, y, dims=[2,3,5]):\n",
        "    for dim in dims:\n",
        "        tsne = TSNE(n_components=dim, perplexity=30, random_state=42)\n",
        "        X_embedded = tsne.fit_transform(X)\n",
        "        clustering = DBSCAN(eps=3, min_samples=5).fit(X_embedded)\n",
        "\n",
        "        plt.figure(figsize=(8,6))\n",
        "        plt.scatter(X_embedded[:,0], X_embedded[:,1], c=clustering.labels_, cmap='tab20', s=5)\n",
        "        plt.title(f\"DBSCAN Clusters on MNIST t-SNE {dim}D\")\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "run_dbscan_on_tsne(X_mnist, y_mnist)"
      ],
      "metadata": {
        "id": "lY7GffQP79GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2: Implement t-SNE dim reduction, run on MNIST Dataset**"
      ],
      "metadata": {
        "id": "6W-8te2C8EHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_tsne(X, n_samples=1000, n_components=2, perplexity=30):\n",
        "    # Sample data\n",
        "    X_sample = X[:n_samples]\n",
        "\n",
        "    # Preprocess data\n",
        "    X_sample = (X_sample - np.mean(X_sample, axis=0)) / np.std(X_sample, axis=0)\n",
        "\n",
        "    # PCA to 50 dimensions\n",
        "    pca = PCA(n_components=50)\n",
        "    X_pca = pca.fit_transform(X_sample)\n",
        "\n",
        "    # Run t-SNE\n",
        "    tsne = TSNE(n_components=n_components, perplexity=perplexity, random_state=42)\n",
        "    X_embedded = tsne.fit_transform(X_pca)\n",
        "\n",
        "    plt.scatter(X_embedded[:,0], X_embedded[:,1], s=5)\n",
        "    plt.title('Custom t-SNE implementation on MNIST')\n",
        "    plt.show()\n",
        "\n",
        "custom_tsne(X_mnist)"
      ],
      "metadata": {
        "id": "xr_ncLpy8FwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3: Pairwise Feature selection for text**\n",
        "\n",
        "On 20NG, run feature selection using skikit-learn built in \"chi2\" criteria to select top 200 features. Rerun a classification task, compare performance with HW3A-PB1. Then repeat the whole pipeline with \"mutual-information\" criteria."
      ],
      "metadata": {
        "id": "S1mo-ASo8VwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for feature selection and classification\n",
        "def feature_selection_and_classification(X, y, method='chi2'):\n",
        "    selector = SelectKBest(score_func=chi2 if method=='chi2' else mutual_info_classif, k=200)\n",
        "    X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "    clf = LogisticRegression(max_iter=500)\n",
        "    clf.fit(X_selected, y)\n",
        "    y_pred = clf.predict(X_selected)\n",
        "\n",
        "    print(f\"Classification Report using {method} feature selection:\")\n",
        "    print(classification_report(y, y_pred))\n",
        "\n",
        "# Original vectorized text data\n",
        "X_text = vectorizer.fit_transform(newsgroups.data[:2000]).toarray()\n",
        "y_text = newsgroups.target[:2000]\n",
        "\n",
        "# Chi2 selection\n",
        "feature_selection_and_classification(X_text, y_text, method='chi2')\n",
        "\n",
        "# Mutual Information selection\n",
        "feature_selection_and_classification(X_text, y_text, method='mutual_info')"
      ],
      "metadata": {
        "id": "GKIUZzzj8d8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 4: L1 feature selection on text**\n",
        "\n",
        "Run a strongL1-regularized regression (library) on 20NG, and select 200 features (words) based on regression coefficients absolute value. Then reconstruct the dateaset with only these features, and rerun any of the classification tasks,"
      ],
      "metadata": {
        "id": "LLlfLUZ18hSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L1-regularized logistic regression for feature selection\n",
        "def l1_feature_selection(X, y, top_features=200):\n",
        "    clf = LogisticRegression(penalty='l1', solver='liblinear', max_iter=500)\n",
        "    clf.fit(X, y)\n",
        "\n",
        "    # Select top features based on coefficient magnitude\n",
        "    importance = np.abs(clf.coef_).sum(axis=0)\n",
        "    top_feature_indices = np.argsort(importance)[-top_features:]\n",
        "\n",
        "    X_selected = X[:, top_feature_indices]\n",
        "\n",
        "    # Re-run classification\n",
        "    clf_selected = LogisticRegression(max_iter=500)\n",
        "    clf_selected.fit(X_selected, y)\n",
        "    y_pred = clf_selected.predict(X_selected)\n",
        "\n",
        "    print(\"Classification report after L1 feature selection:\")\n",
        "    print(classification_report(y, y_pred))\n",
        "\n",
        "l1_feature_selection(X_text, y_text)"
      ],
      "metadata": {
        "id": "uwC9a_598nmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hnToEx5H8qGz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}