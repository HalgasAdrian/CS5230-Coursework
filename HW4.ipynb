{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyP/x+rWNUYwpHi7bbU28n9V"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "HW4 Pytorch: Classification, Autoencoders, Word Embedding, Image Features, LSTM\n",
        "\n",
        "PROBLEM 1: Setup a tensor library [Optional, no credit]\n",
        "\n",
        "A) Setup your favorite tensor-based library for deep learning, such as PyTorch or TensorFlow, and familiarize yourself with its basic usage. If using PyTorch, you can test if it is installed properly with (in Python):\n",
        "\n",
        "B) Train a simple feed-forward neural network on the MNIST dataset with 80/20 train and test splits and report results\n",
        "\n"
      ],
      "metadata": {
        "id": "e79WDeGL5KHO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9RNI1Gxm39_T",
        "outputId": "0e995f4b-16b5-4f7d-d61c-009aa1f7ecd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 219.8138\n",
            "Epoch 2, Loss: 52.8084\n",
            "Epoch 3, Loss: 36.8860\n",
            "Epoch 4, Loss: 29.0962\n",
            "Epoch 5, Loss: 22.8169\n",
            "Test Accuracy: 98.77%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Device config\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "epochs = 5  # You can increase for better accuracy\n",
        "\n",
        "# MNIST dataset (download + transform)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load full dataset\n",
        "full_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# 80/20 split\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Neural Net Model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.dropout = nn.Dropout2d(0.25)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = F.max_pool2d(x, 2)            # Pool after conv1\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = F.max_pool2d(x, 2)            # Pool after conv2\n",
        "      x = self.dropout(x)\n",
        "      x = torch.flatten(x, 1)           # Flatten\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Optimizer and Loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, nesterov=True)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        data, target = batch\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += (pred == target).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "accuracy = 100. * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 2 : NNet supervised classification with tuned word vectors\n",
        "\n",
        "Train a neural network on a sizeable subset of 20NG (say, at least 5 categories)\n"
      ],
      "metadata": {
        "id": "YwfTwfbz5otR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup and preprocessing:"
      ],
      "metadata": {
        "id": "8NM9MKOFpK6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "categories = ['comp.graphics', 'sci.space', 'rec.sport.baseball', 'talk.politics.misc', 'soc.religion.christian']\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
        "texts, labels = newsgroups.data, newsgroups.target"
      ],
      "metadata": {
        "id": "OxLzaoaM5sYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_texts = [simple_preprocess(text) for text in texts]"
      ],
      "metadata": {
        "id": "88ZiJ9uuo1eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load GloVe and create Vocabulary"
      ],
      "metadata": {
        "id": "gd1p-534pNYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "#Building vocab from corpus\n",
        "word_counts = Counter(word for doc in tokenized_texts for word in doc)\n",
        "vocab = {word: i+2 for i, (word, _) in enumerate(word_counts.items())}\n",
        "vocab['<PAD>'] = 0\n",
        "vocab['<UNK>'] = 1"
      ],
      "metadata": {
        "id": "z6B_9r7io3hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Loading GloVe embeddings\n",
        "embedding_dim = 100\n",
        "glove_path = 'glove.6B.100d.txt'\n",
        "embeddings_index = {}\n",
        "\n",
        "with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = vector"
      ],
      "metadata": {
        "id": "lAa1OTrwo47J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Embedding Matrix\n",
        "embedding_matrix = np.random.normal(0, 1, (len(vocab), embedding_dim))\n",
        "for word, idx in vocab.items():\n",
        "    if word in embeddings_index:\n",
        "        embedding_matrix[idx] = embeddings_index[word]"
      ],
      "metadata": {
        "id": "2onE4Q5zo6pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting Docs to Indices"
      ],
      "metadata": {
        "id": "GefFY6T-peAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def doc2ind(doc):\n",
        "    return [vocab.get(token, vocab['<UNK>']) for token in doc]\n",
        "\n",
        "indexed_docs = [doc2ind(doc) for doc in tokenized_texts]"
      ],
      "metadata": {
        "id": "EqpWbDjMo70C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "padded_docs = pad_sequence([torch.tensor(doc) for doc in indexed_docs], batch_first=True, padding_value=vocab['<PAD>'])\n",
        "labels_tensor = torch.tensor(labels)"
      ],
      "metadata": {
        "id": "nL-W98auo87U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataset and Dataloader"
      ],
      "metadata": {
        "id": "U9AdU6IYpnZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "dataset = TensorDataset(padded_docs, labels_tensor)\n",
        "train_len = int(0.8 * len(dataset))\n",
        "train_ds, val_ds = random_split(dataset, [train_len, len(dataset) - train_len])\n",
        "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=64)"
      ],
      "metadata": {
        "id": "A_TSFQD5o-HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define our Neural Network"
      ],
      "metadata": {
        "id": "QgJZsWCDpqaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=True)\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, embedding_layer, hidden_dim, num_classes):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.embedding = embedding_layer\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.mean(dim=1)  # mean pooling\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return torch.softmax(x, dim=1)\n",
        "\n",
        "model = TextClassifier(embedding_layer, hidden_dim=128, num_classes=len(categories))"
      ],
      "metadata": {
        "id": "J15aoPJ5pAB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "GInO4G-Kp3iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in train_dl:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "id": "yDmHeN3DpBS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate accuracy"
      ],
      "metadata": {
        "id": "NMDa8iKTp5hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for x_batch, y_batch in val_dl:\n",
        "        y_pred = model(x_batch)\n",
        "        predicted = y_pred.argmax(dim=1)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "print(f\"Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "MqLH-iJppCx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fine tuning the embeddings\n",
        "model.embedding.weight.requires_grad = True"
      ],
      "metadata": {
        "id": "X8df-7z3pEJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of embeddings using TSNE"
      ],
      "metadata": {
        "id": "63lMrEVxp-V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get a sample of word vectors\n",
        "sample_idx = [vocab[word] for word in list(vocab.keys())[:500]]\n",
        "sample_vectors = model.embedding.weight.data[sample_idx].cpu().numpy()\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "reduced = tsne.fit_transform(sample_vectors)\n",
        "\n",
        "plt.scatter(reduced[:, 0], reduced[:, 1])\n",
        "plt.title(\"t-SNE of Word Embeddings (Post-Fine-Tuning)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F5LzxUUPpFe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 3 [Optional, no credit]: Autoencoders\n",
        "\n",
        "You can pick your own text to fine tune word vectors, if its reasonable in size and very domain-specific (compared to general English). Suggestions:\n",
        "- Alice in Wonderland\n",
        "- Sonnets\n",
        "- specific categories (labels) from 20NG or Reurters datasets\n",
        "- use your favorite specific text (like a book, or project)\n"
      ],
      "metadata": {
        "id": "l1j-1GDn5s82"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BsUvzwxP5wbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 4 [Optional, no credit]: Autoencoders\n",
        "\n",
        "For each one of the datasets MNIST, 20NG (required) and SPAMBASE, FASHION (optional) run as an autoencoder with pytorch with a desired hidden layer size (try K=5,10, 20, 100, 200)- what is the smaleest K that works?).\n",
        "\n",
        "Load the data with dataloader https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "\n",
        "Construct an Autoencoder with the following architecture :\n",
        "\n",
        "Two linear layers with in features matching the dimensions of input and out\n",
        "\n",
        "features matching the size of K\n",
        "\n",
        "Two linear layers with in features matching K and size of out features matching the size of input dimensions.\n",
        "\n",
        "Define a forward pass with relu\n",
        "\n",
        "Code a train loop with number of epochs as 10.\n",
        "\n",
        "Define loss and Optimizer (Adam)\n",
        "\n",
        "Train the model\n",
        "\n",
        "use gpu if available\n",
        "\n",
        "use mean-squared error loss\n",
        "\n",
        "create a model from Autoencoder class load it to the specified device, either gpu or cpu\n",
        "\n",
        "Verify the obtained re-encoding of data (the new feature representation) in several ways:\n",
        "\n",
        "repeat a classification train/test task , or a clustering taks\n",
        "examine the new pairwise distances dist(i,j) agains the old distances obtained with original features (sample 100 pairs of related words)\n",
        "examine the top 20 neighbors (by new distance) set overlap with old neighbors, per datapoint\n",
        "for images, rebuild the image form output layer and draw to look at it\n"
      ],
      "metadata": {
        "id": "2RUiT9_w5ytm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2O0PNpMQ6EDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 5 [Optional, no credit]: Image Feature Extraction\n",
        "\n",
        "Run a Convolutional Neural Network in pytorch to extract image features. In practice the network usually does both the feature extraction and the supervised task (classification) in one pipeline.\n"
      ],
      "metadata": {
        "id": "MqknZXDI6Hak"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uCITaHzP6JzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 6 [Optional, no credit]: LSTM for text\n",
        "\n",
        "Run a Recurrent Neural Network/LSTM in Pytorch to model word dependecies/order in text. Can be use for translation, next-word prediction, event detection etc.\n"
      ],
      "metadata": {
        "id": "mrTmQwMs6LQa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tl4bvyxO6N5W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}