{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFDgx+F1GuLpLcLHLcPU+w"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1: Supervised Classification Libraries: Regression, Decision Tree**\n",
        "\n",
        "6 Runs of Supervised Training / Testing : 3 datasets (MNIST, Spambase, 20NG) x 2 Classification Algorithms (L2-reg Logistic Regression, Decision Trees). You can use a library for the classification algorithms, and also can use any library/script to process data in appropriate formats.\n",
        "You are required to explain/analyze the model trained in terms of features : for each of the 6 runs list the top F=30 features. For the Regression these correspond to the highest-absolute-value F coefficients; for Decision Tree they are the first F splits. In particular for Decision Tree on 20NG, report performance for two tree sizes ( by depths of the tree, or number of leaves, or number of splits )"
      ],
      "metadata": {
        "id": "wGU9SzNy9RkY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jTU5nQcd8b9k",
        "outputId": "f5b9e6b7-e263-4de6-d959-bd05f85e73da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MNIST dataset...\n",
            "\n",
            "=== MNIST ===\n",
            "\n",
            "-- Logistic Regression --\n",
            "Accuracy: 0.9155\n",
            "Top 30 features (by coefficient magnitude):\n",
            "Feature: pixel_379, Coefficient: -2.140631880246161\n",
            "Feature: pixel_517, Coefficient: -1.724224949896848\n",
            "Feature: pixel_710, Coefficient: -1.6411961929051553\n",
            "Feature: pixel_305, Coefficient: -1.5838818330965132\n",
            "Feature: pixel_461, Coefficient: -1.5161776873412551\n",
            "Feature: pixel_712, Coefficient: -1.4760219309135363\n",
            "Feature: pixel_715, Coefficient: -1.4701182900734062\n",
            "Feature: pixel_222, Coefficient: -1.4373566837046465\n",
            "Feature: pixel_718, Coefficient: -1.41116516174984\n",
            "Feature: pixel_177, Coefficient: -1.3643682506328025\n",
            "Feature: pixel_249, Coefficient: -1.3559286839744613\n",
            "Feature: pixel_102, Coefficient: -1.346720288565536\n",
            "Feature: pixel_489, Coefficient: -1.3217004157337955\n",
            "Feature: pixel_192, Coefficient: -1.315472231315428\n",
            "Feature: pixel_714, Coefficient: -1.2855499217760067\n",
            "Feature: pixel_250, Coefficient: -1.2788168791521628\n",
            "Feature: pixel_164, Coefficient: -1.2676271392498357\n",
            "Feature: pixel_333, Coefficient: -1.2472189254333672\n",
            "Feature: pixel_93, Coefficient: -1.2380606228458384\n",
            "Feature: pixel_717, Coefficient: -1.2075194693955398\n",
            "Feature: pixel_713, Coefficient: -1.1831573854741764\n",
            "Feature: pixel_648, Coefficient: -1.1736180416704902\n",
            "Feature: pixel_406, Coefficient: -1.173252919625437\n",
            "Feature: pixel_221, Coefficient: -1.1561448081612502\n",
            "Feature: pixel_629, Coefficient: 1.1527471907251328\n",
            "Feature: pixel_193, Coefficient: -1.1448527790823544\n",
            "Feature: pixel_434, Coefficient: -1.139573304151059\n",
            "Feature: pixel_544, Coefficient: 1.1201527427637037\n",
            "Feature: pixel_605, Coefficient: -1.1110598596121202\n",
            "Feature: pixel_709, Coefficient: -1.0989962839243381\n",
            "\n",
            "-- Decision Tree --\n",
            "Accuracy: 0.8683\n",
            "Top 30 splits (in order of appearance):\n",
            "Split 1: Feature: pixel_350\n",
            "Split 2: Feature: pixel_435\n",
            "Split 3: Feature: pixel_489\n",
            "Split 4: Feature: pixel_597\n",
            "Split 5: Feature: pixel_542\n",
            "Split 6: Feature: pixel_290\n",
            "Split 7: Feature: pixel_347\n",
            "Split 8: Feature: pixel_486\n",
            "Split 9: Feature: pixel_489\n",
            "Split 10: Feature: pixel_432\n",
            "Split 11: Feature: pixel_270\n",
            "Split 12: Feature: pixel_486\n",
            "Split 13: Feature: pixel_297\n",
            "Split 14: Feature: pixel_206\n",
            "Split 15: Feature: pixel_657\n",
            "Split 16: Feature: pixel_404\n",
            "Split 17: Feature: pixel_571\n",
            "Split 18: Feature: pixel_380\n",
            "Split 19: Feature: pixel_347\n",
            "Split 20: Feature: pixel_486\n",
            "Split 21: Feature: pixel_211\n",
            "Split 22: Feature: pixel_273\n",
            "Split 23: Feature: pixel_347\n",
            "Split 24: Feature: pixel_296\n",
            "Split 25: Feature: pixel_657\n",
            "Split 26: Feature: pixel_486\n",
            "Split 27: Feature: pixel_626\n",
            "Split 28: Feature: pixel_457\n",
            "Split 29: Feature: pixel_542\n",
            "Split 30: Feature: pixel_574\n",
            "Loading Spambase dataset...\n",
            "\n",
            "=== Spambase ===\n",
            "\n",
            "-- Logistic Regression --\n",
            "Accuracy: 0.9319\n",
            "Top 30 features (by coefficient magnitude):\n",
            "Feature: feature_26, Coefficient: -3.765655590899128\n",
            "Feature: feature_52, Coefficient: 3.4119505680331392\n",
            "Feature: feature_22, Coefficient: 2.1412860264654214\n",
            "Feature: feature_6, Coefficient: 2.0486278898997243\n",
            "Feature: feature_24, Coefficient: -1.7761431270017602\n",
            "Feature: feature_43, Coefficient: -1.727172076869604\n",
            "Feature: feature_41, Coefficient: -1.4635741420251351\n",
            "Feature: feature_47, Coefficient: -1.4314379963605042\n",
            "Feature: feature_14, Coefficient: 1.342359803369464\n",
            "Feature: feature_45, Coefficient: -1.3147774849742841\n",
            "Feature: feature_32, Coefficient: -1.1738162473304705\n",
            "Feature: feature_28, Coefficient: -1.1732903288825016\n",
            "Feature: feature_40, Coefficient: -1.1697318774304057\n",
            "Feature: feature_8, Coefficient: 1.146958341704164\n",
            "Feature: feature_48, Coefficient: -1.076830852187488\n",
            "Feature: feature_15, Coefficient: 1.068857411527887\n",
            "Feature: feature_53, Coefficient: 1.0206897945965197\n",
            "Feature: feature_35, Coefficient: 0.9840507570816585\n",
            "Feature: feature_16, Coefficient: 0.9839644756424769\n",
            "Feature: feature_19, Coefficient: 0.9262279772229268\n",
            "Feature: feature_34, Coefficient: -0.9112972118011899\n",
            "Feature: feature_25, Coefficient: -0.8367382024958062\n",
            "Feature: feature_38, Coefficient: -0.7763066097014767\n",
            "Feature: feature_3, Coefficient: 0.7348566856300985\n",
            "Feature: feature_29, Coefficient: -0.6764203830054736\n",
            "Feature: feature_44, Coefficient: -0.6643087324753139\n",
            "Feature: feature_42, Coefficient: -0.629950569103837\n",
            "Feature: feature_4, Coefficient: 0.5999758549592045\n",
            "Feature: feature_46, Coefficient: -0.586378894683841\n",
            "Feature: feature_37, Coefficient: -0.5861571257337095\n",
            "\n",
            "-- Decision Tree --\n",
            "Accuracy: 0.8993\n",
            "Top 30 splits (in order of appearance):\n",
            "Split 1: Feature: feature_52\n",
            "Split 2: Feature: feature_6\n",
            "Split 3: Feature: feature_24\n",
            "Split 4: Feature: feature_51\n",
            "Split 5: Feature: feature_26\n",
            "Split 6: Feature: feature_45\n",
            "Split 7: Feature: feature_6\n",
            "Split 8: Feature: feature_15\n",
            "Split 9: Feature: feature_54\n",
            "Split 10: Feature: feature_40\n",
            "Split 11: Feature: feature_55\n",
            "Split 12: Feature: feature_22\n",
            "Split 13: Feature: feature_54\n",
            "Split 14: Feature: feature_4\n",
            "Split 15: Feature: feature_15\n",
            "Split 16: Feature: feature_34\n",
            "Split 17: Feature: feature_9\n",
            "Split 18: Feature: feature_6\n",
            "Split 19: Feature: feature_26\n",
            "Split 20: Feature: feature_24\n",
            "Split 21: Feature: feature_4\n",
            "Split 22: Feature: feature_18\n",
            "Split 23: Feature: feature_18\n",
            "Split 24: Feature: feature_16\n",
            "Split 25: Feature: feature_2\n",
            "Split 26: Feature: feature_38\n",
            "Split 27: Feature: feature_24\n",
            "Split 28: Feature: feature_18\n",
            "Split 29: Feature: feature_25\n",
            "Split 30: Feature: feature_7\n",
            "Loading 20 Newsgroups dataset...\n",
            "\n",
            "=== 20 Newsgroups (Logistic Regression) ===\n",
            "Accuracy: 0.6203\n",
            "Top 30 features (by coefficient magnitude):\n",
            "Feature: religion, Coefficient: 3.9551969019486006\n",
            "Feature: atheist, Coefficient: 3.6967843434175958\n",
            "Feature: atheism, Coefficient: 3.477685266238655\n",
            "Feature: islam, Coefficient: 3.4421821454419734\n",
            "Feature: atheists, Coefficient: 3.082248449642043\n",
            "Feature: god, Coefficient: 2.5538728417732703\n",
            "Feature: with, Coefficient: -2.3362755319992003\n",
            "Feature: moral, Coefficient: 2.131058199602209\n",
            "Feature: said, Coefficient: 2.063505640154118\n",
            "Feature: stay, Coefficient: 1.9974033432318479\n",
            "Feature: morality, Coefficient: 1.972940462772378\n",
            "Feature: religious, Coefficient: 1.9558119238476708\n",
            "Feature: argument, Coefficient: 1.871515199189096\n",
            "Feature: what, Coefficient: 1.791148183624199\n",
            "Feature: exist, Coefficient: 1.7509539577119326\n",
            "Feature: belief, Coefficient: 1.7467574180392258\n",
            "Feature: agree, Coefficient: 1.7441099197890968\n",
            "Feature: use, Coefficient: -1.741117059717193\n",
            "Feature: society, Coefficient: 1.734945407517858\n",
            "Feature: book, Coefficient: 1.6905489934284017\n",
            "Feature: bible, Coefficient: 1.6892551662908648\n",
            "Feature: bob, Coefficient: 1.653805231158234\n",
            "Feature: post, Coefficient: 1.643960051858285\n",
            "Feature: tells, Coefficient: 1.6255638893404007\n",
            "Feature: also, Coefficient: -1.6172383051257808\n",
            "Feature: objective, Coefficient: 1.5797455877408668\n",
            "Feature: is, Coefficient: 1.5712200307859998\n",
            "Feature: you, Coefficient: 1.5512770598333705\n",
            "Feature: claim, Coefficient: 1.5438019706878294\n",
            "Feature: away, Coefficient: 1.53320257932526\n",
            "\n",
            "=== 20 Newsgroups (Decision Tree) ===\n",
            "\n",
            "-- Decision Tree (Shallow: max_depth=5) --\n",
            "Accuracy: 0.1500\n",
            "Top 30 splits (Shallow Tree):\n",
            "Split 1: Feature: god\n",
            "Split 2: Feature: windows\n",
            "Split 3: Feature: christ\n",
            "Split 4: Feature: bike\n",
            "Split 5: Feature: sale\n",
            "Split 6: Feature: church\n",
            "Split 7: Feature: jesus\n",
            "Split 8: Feature: sale\n",
            "Split 9: Feature: please\n",
            "Split 10: Feature: server\n",
            "Split 11: Feature: door\n",
            "Split 12: Feature: his\n",
            "Split 13: Feature: government\n",
            "Split 14: Feature: and\n",
            "Split 15: Feature: about\n",
            "Split 16: Feature: car\n",
            "Split 17: Feature: that\n",
            "Split 18: Feature: 12\n",
            "Split 19: Feature: comp\n",
            "Split 20: Feature: won\n",
            "Split 21: Feature: atheism\n",
            "Split 22: Feature: said\n",
            "Split 23: Feature: your\n",
            "Split 24: Feature: folks\n",
            "Split 25: Feature: totally\n",
            "Split 26: Feature: were\n",
            "Split 27: Feature: still\n",
            "Split 28: Feature: cc\n",
            "\n",
            "-- Decision Tree (Deep: max_depth=15) --\n",
            "Accuracy: 0.2646\n",
            "Top 30 splits (Deep Tree):\n",
            "Split 1: Feature: god\n",
            "Split 2: Feature: windows\n",
            "Split 3: Feature: christ\n",
            "Split 4: Feature: bike\n",
            "Split 5: Feature: sale\n",
            "Split 6: Feature: church\n",
            "Split 7: Feature: jesus\n",
            "Split 8: Feature: sale\n",
            "Split 9: Feature: please\n",
            "Split 10: Feature: server\n",
            "Split 11: Feature: door\n",
            "Split 12: Feature: his\n",
            "Split 13: Feature: government\n",
            "Split 14: Feature: and\n",
            "Split 15: Feature: about\n",
            "Split 16: Feature: car\n",
            "Split 17: Feature: that\n",
            "Split 18: Feature: 12\n",
            "Split 19: Feature: comp\n",
            "Split 20: Feature: return\n",
            "Split 21: Feature: atheism\n",
            "Split 22: Feature: said\n",
            "Split 23: Feature: your\n",
            "Split 24: Feature: reason\n",
            "Split 25: Feature: totally\n",
            "Split 26: Feature: were\n",
            "Split 27: Feature: still\n",
            "Split 28: Feature: stop\n",
            "Split 29: Feature: encryption\n",
            "Split 30: Feature: condition\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "from sklearn.datasets import fetch_openml, fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "##############################################\n",
        "# Helper functions for feature extraction\n",
        "##############################################\n",
        "\n",
        "def get_top_features_logistic(model, feature_names=None, F=30):\n",
        "    \"\"\"\n",
        "    For logistic regression: Returns the top F features\n",
        "    sorted by absolute value of the coefficients.\n",
        "    If feature_names is provided, return names with coefficients.\n",
        "    \"\"\"\n",
        "    # For binary classification, coef_ is shape (1, n_features)\n",
        "    coef = model.coef_[0]\n",
        "    abs_coef = np.abs(coef)\n",
        "    top_indices = np.argsort(abs_coef)[-F:][::-1]\n",
        "    if feature_names is not None:\n",
        "        top_features = [(feature_names[i], coef[i]) for i in top_indices]\n",
        "    else:\n",
        "        top_features = [(i, coef[i]) for i in top_indices]\n",
        "    return top_features\n",
        "\n",
        "def get_top_splits(decision_tree, F=30):\n",
        "    \"\"\"\n",
        "    For decision trees: Traverse the tree in breadth-first order\n",
        "    and return the feature indices for the first F splits.\n",
        "    Leaf nodes have feature index -2.\n",
        "    \"\"\"\n",
        "    tree = decision_tree.tree_\n",
        "    q = deque([0])  # Start from the root node (index 0)\n",
        "    splits = []\n",
        "    while q and len(splits) < F:\n",
        "        node = q.popleft()\n",
        "        # Check if current node is a split (non-leaf)\n",
        "        if tree.feature[node] != -2:\n",
        "            splits.append(tree.feature[node])\n",
        "            # Enqueue children nodes (if they exist)\n",
        "            left_child = tree.children_left[node]\n",
        "            right_child = tree.children_right[node]\n",
        "            if left_child != -1:\n",
        "                q.append(left_child)\n",
        "            if right_child != -1:\n",
        "                q.append(right_child)\n",
        "    return splits\n",
        "\n",
        "##############################################\n",
        "# General classification function for a dataset\n",
        "##############################################\n",
        "\n",
        "def run_classification(X, y, feature_names=None, dataset_name=\"Dataset\"):\n",
        "    print(f\"\\n=== {dataset_name} ===\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    ##############################################\n",
        "    # Logistic Regression\n",
        "    ##############################################\n",
        "    print(\"\\n-- Logistic Regression --\")\n",
        "    lr = LogisticRegression(penalty='l2', solver='liblinear', max_iter=1000)\n",
        "    lr.fit(X_train, y_train)\n",
        "    y_pred_lr = lr.predict(X_test)\n",
        "    acc_lr = accuracy_score(y_test, y_pred_lr)\n",
        "    print(f\"Accuracy: {acc_lr:.4f}\")\n",
        "    top_lr_features = get_top_features_logistic(lr, feature_names)\n",
        "    print(\"Top 30 features (by coefficient magnitude):\")\n",
        "    for feature, coef in top_lr_features:\n",
        "        print(f\"Feature: {feature}, Coefficient: {coef}\")\n",
        "\n",
        "    ##############################################\n",
        "    # Decision Tree\n",
        "    ##############################################\n",
        "    print(\"\\n-- Decision Tree --\")\n",
        "    dt = DecisionTreeClassifier(random_state=42)\n",
        "    dt.fit(X_train, y_train)\n",
        "    y_pred_dt = dt.predict(X_test)\n",
        "    acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "    print(f\"Accuracy: {acc_dt:.4f}\")\n",
        "    top_dt_splits = get_top_splits(dt)\n",
        "    print(\"Top 30 splits (in order of appearance):\")\n",
        "    for i, feat in enumerate(top_dt_splits):\n",
        "        if feature_names is not None and feat < len(feature_names):\n",
        "            feat_name = feature_names[feat]\n",
        "        else:\n",
        "            feat_name = feat\n",
        "        print(f\"Split {i+1}: Feature: {feat_name}\")\n",
        "\n",
        "##############################################\n",
        "# MNIST: Load and run experiments\n",
        "##############################################\n",
        "\n",
        "def run_mnist():\n",
        "    print(\"Loading MNIST dataset...\")\n",
        "    # MNIST from openml; scale pixel values to [0,1]\n",
        "    mnist = fetch_openml('mnist_784', version=1)\n",
        "    X = mnist.data.astype(np.float32) / 255.0\n",
        "    y = mnist.target.astype(np.int64)\n",
        "    # Create feature names for pixels\n",
        "    feature_names = [f\"pixel_{i}\" for i in range(X.shape[1])]\n",
        "    run_classification(X, y, feature_names, dataset_name=\"MNIST\")\n",
        "\n",
        "##############################################\n",
        "# Spambase: Load and run experiments\n",
        "##############################################\n",
        "\n",
        "def run_spambase():\n",
        "    print(\"Loading Spambase dataset...\")\n",
        "    # Assumes 'spambase.data' is in the current directory.\n",
        "    # The dataset is comma-separated, with the last column as the target.\n",
        "    data = np.loadtxt(\"spambase.data\", delimiter=\",\")\n",
        "    X = data[:, :-1]\n",
        "    y = data[:, -1]\n",
        "    # Create feature names for spambase features\n",
        "    feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
        "    run_classification(X, y, feature_names, dataset_name=\"Spambase\")\n",
        "\n",
        "##############################################\n",
        "# 20 Newsgroups: Load and run experiments\n",
        "##############################################\n",
        "\n",
        "def run_20newsgroups():\n",
        "    print(\"Loading 20 Newsgroups dataset...\")\n",
        "    # Load train and test splits; remove headers/footers/quotes to focus on content.\n",
        "    newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "    newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "    # Use TfidfVectorizer to convert text to feature vectors (limit features for speed)\n",
        "    vectorizer = TfidfVectorizer(max_features=2000)\n",
        "    X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "    X_test = vectorizer.transform(newsgroups_test.data)\n",
        "    # Combine train and test to allow our own splitting\n",
        "    from scipy.sparse import vstack\n",
        "    X = vstack([X_train, X_test])\n",
        "    y = np.concatenate([newsgroups_train.target, newsgroups_test.target])\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    ##############################################\n",
        "    # Logistic Regression on 20NG\n",
        "    ##############################################\n",
        "    print(\"\\n=== 20 Newsgroups (Logistic Regression) ===\")\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    lr = LogisticRegression(penalty='l2', solver='liblinear', max_iter=1000)\n",
        "    lr.fit(X_tr, y_tr)\n",
        "    y_pred_lr = lr.predict(X_te)\n",
        "    acc_lr = accuracy_score(y_te, y_pred_lr)\n",
        "    print(f\"Accuracy: {acc_lr:.4f}\")\n",
        "    top_lr_features = get_top_features_logistic(lr, feature_names)\n",
        "    print(\"Top 30 features (by coefficient magnitude):\")\n",
        "    for feature, coef in top_lr_features:\n",
        "        print(f\"Feature: {feature}, Coefficient: {coef}\")\n",
        "\n",
        "    ##############################################\n",
        "    # Decision Tree on 20NG with two configurations\n",
        "    ##############################################\n",
        "    print(\"\\n=== 20 Newsgroups (Decision Tree) ===\")\n",
        "\n",
        "    # Configuration 1: Shallow Tree (max_depth=5)\n",
        "    print(\"\\n-- Decision Tree (Shallow: max_depth=5) --\")\n",
        "    dt_shallow = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "    dt_shallow.fit(X_tr, y_tr)\n",
        "    y_pred_shallow = dt_shallow.predict(X_te)\n",
        "    acc_shallow = accuracy_score(y_te, y_pred_shallow)\n",
        "    print(f\"Accuracy: {acc_shallow:.4f}\")\n",
        "    top_splits_shallow = get_top_splits(dt_shallow)\n",
        "    print(\"Top 30 splits (Shallow Tree):\")\n",
        "    for i, feat in enumerate(top_splits_shallow):\n",
        "        feat_name = feature_names[feat] if feat < len(feature_names) else feat\n",
        "        print(f\"Split {i+1}: Feature: {feat_name}\")\n",
        "\n",
        "    # Configuration 2: Deep Tree (max_depth=15)\n",
        "    print(\"\\n-- Decision Tree (Deep: max_depth=15) --\")\n",
        "    dt_deep = DecisionTreeClassifier(max_depth=15, random_state=42)\n",
        "    dt_deep.fit(X_tr, y_tr)\n",
        "    y_pred_deep = dt_deep.predict(X_te)\n",
        "    acc_deep = accuracy_score(y_te, y_pred_deep)\n",
        "    print(f\"Accuracy: {acc_deep:.4f}\")\n",
        "    top_splits_deep = get_top_splits(dt_deep)\n",
        "    print(\"Top 30 splits (Deep Tree):\")\n",
        "    for i, feat in enumerate(top_splits_deep):\n",
        "        feat_name = feature_names[feat] if feat < len(feature_names) else feat\n",
        "        print(f\"Split {i+1}: Feature: {feat_name}\")\n",
        "\n",
        "##############################################\n",
        "# Main execution: run all experiments\n",
        "##############################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run experiments on MNIST\n",
        "    run_mnist()\n",
        "\n",
        "    # Run experiments on Spambase\n",
        "    # Ensure that \"spambase.data\" is uploaded to your Colab environment.\n",
        "    run_spambase()\n",
        "\n",
        "    # Run experiments on 20 Newsgroups\n",
        "    run_20newsgroups()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2: PCA library on MNIST**\n",
        "\n",
        "A) For MNIST dataset, run a PCA-library to get data on D=5 features. Rerun the classification tasks from PB1, compare testing performance with the one from PB1. Then repeat this exercise for D=20\n",
        "B) Run PCA library on Spambase and repeat one of the classification algorithms. What is the smallest D (number of PCA dimensions) you need to get a comparable test result?"
      ],
      "metadata": {
        "id": "qciU_0Jl9buW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EahmIliS9fO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3: Implement PCA on MNIST**\n",
        "\n",
        "Repeat PB2 exercises on MNIST (D=5 and D=20) with your own PCA implementation. You can use any built-in library/package/API for : matrix storage/multiplication, covariance computation, eigenvalue or SVD decomposition, etc. Matlab is probably the easiest language for implementing PCA due to its excellent linear algebra support.\n"
      ],
      "metadata": {
        "id": "LoP27Zqt9jUj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cbjvQoEC9n-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 4: PCA for clustering visualization**\n",
        "\n",
        "A) Run KMeans on MNIST data (or a sample of it)\n",
        "B) Run PCA on same data\n",
        "C) Plot data in 3D with PCA representation with t=3 top eigen values; use shapes to to indicate truth digit label (circle, triangle, \"+\", stars, etc) and colors to indicate cluster ID (red blue green etc).\n",
        "D) Select other 3 at random eigen values from top 20; redo the plot several times."
      ],
      "metadata": {
        "id": "NU_6HaFu9tqg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7rModEuA9z2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 5: Implement Kernel PCA for Logistic Regression**\n",
        "\n",
        "Dataset: 1000 2-dim datapoints TwoSpirals\n",
        "Dataset: 1000 2-dim datapoints ThreeCircles\n",
        "\n",
        "A) First, train a Linear/Logistic Regression (library, logistic if data labels are categories) and confirm that it doesnt work , i.e. it has a high classification error or high Root Mean Squared Error.\n",
        "B) Run KernelPCA with Gaussian Kernel to obtain a representation of T features. For reference these steps we demoed in class (Matlab):\n",
        "%get pairwise squared euclidian distance\n",
        "X2 = dot(X,X,2);\n",
        "DIST_euclid = bsxfun(@plus, X2, X2') - 2 * X * X';\n",
        "% get a kernel matrix NxN\n",
        "sigma = 3;\n",
        "K = exp(-DIST_euclid/sigma);\n",
        "%normalize the Kernel to correspond to zero-mean\n",
        "U = ones(N)/ N ;\n",
        "Kn = K - U*K -K*U + U*K*U ;\n",
        "% obtain kernel eignevalues, vectors; then sort them with largest eig first\n",
        "[V,D] = eig(Kn,'vector') ;\n",
        "[D,sorteig] = sort(D,'descend') ;\n",
        "V = V(:, sorteig);\n",
        "% get the projection matrix\n",
        "XG = Kn*V;\n",
        "%get first 3 dimensions\n",
        "X3G = XG(:,1:3);\n",
        "%get first 20 dimensions\n",
        "X20G = XG(:,1:20);\n",
        "%get first 100 dimensions\n",
        "X100G = XG(:,1:100);\n",
        "\n",
        "C) Retrain the regression algorithm on the same data kernelized / dual form. How large T needs to be to get good performance?"
      ],
      "metadata": {
        "id": "LQZh32Qp91_k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x0JTjLFj9-nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROBLEM 6 - OPTIONAL (no credit) : Implement Kernel PCA on MNIST**\n",
        "\n",
        "A) First, add Gaussian noise to MNIST images.\n",
        "B) Then rerun PCA on noisy images (D=5 and D=20) and inspect visually the images obtained by PCA representation\n",
        "C) Run Kernel-PCA with the RBF Kernel (D=5 and D=20) on noisy images and observe better images visually."
      ],
      "metadata": {
        "id": "gO1nzuW_-BGO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mMb8UOvJ-EfJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}