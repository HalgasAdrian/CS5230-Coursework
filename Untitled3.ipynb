{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOqHH0MwhQwEh+OE3JxqDEZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4bHFGLzCjOA"
      },
      "outputs": [],
      "source": [
        "# Final Exam Spring 2025 - Adrian Halgas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1 Clustering Noisy Images**\n",
        "\n",
        "DATATSET 8358 sampled images with not-uniform label distribution. Each row is an image: first column is the label (digit), then the other 784 columns are pixel values. These images have noise in them; to achieve a better result, you need to work on the features first.\n",
        "\n",
        "Task: Run a clustering algorithm on the given data set, extract k=10 clusters, and report entropy statistics using the given evaluation function (or write your own). You will have to decide the data preprocessing (if any) and the clustering algorithm. You can use scientific computing libraries (e.g. NumPy / SciPy) for both processing (for example PCA) and clustering (for example KMeans), and you can use any functions you developed in your previous homeworks.\n",
        "\n",
        "Labels are not to be used during the algorithm/clustering/preprocessing, but only for evaluation: print a confusion matrix of counts, calculate entropy on each row and column, and compute weighted_by_count average entropy for rows (labels) and columns (clusters). Make sure to include all datapoints into the K=10 clusters."
      ],
      "metadata": {
        "id": "gf7AOEXWP4fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "uofBzVheQKFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# My KMeans implementation from HW2A\n",
        "\n",
        "class KMeans:\n",
        "    def __init__(self, n_clusters=10, max_iter=300, tol=1e-4):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.centroids = None\n",
        "        self.labels = None\n",
        "\n",
        "    def fit(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize centroids randomly\n",
        "        random_indices = np.random.choice(n_samples, self.n_clusters, replace=False)\n",
        "        self.centroids = X[random_indices]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            # Assign clusters\n",
        "            distances = self._compute_distances(X)\n",
        "            self.labels = np.argmin(distances, axis=1)\n",
        "\n",
        "            # Update centroids\n",
        "            new_centroids = np.array([X[self.labels == k].mean(axis=0) for k in range(self.n_clusters)])\n",
        "\n",
        "            # Check for convergence\n",
        "            if np.linalg.norm(new_centroids - self.centroids) < self.tol:\n",
        "                break\n",
        "\n",
        "            self.centroids = new_centroids\n",
        "\n",
        "    def _compute_distances(self, X):\n",
        "        distances = np.zeros((X.shape[0], self.n_clusters))\n",
        "        for k in range(self.n_clusters):\n",
        "            distances[:, k] = np.linalg.norm(X - self.centroids[k], axis=1)\n",
        "        return distances\n",
        "\n",
        "    def predict(self, X):\n",
        "        distances = self._compute_distances(X)\n",
        "        return np.argmin(distances, axis=1)"
      ],
      "metadata": {
        "id": "qtpbpxAYSW0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMEANS on 8358 sampled images"
      ],
      "metadata": {
        "id": "1fqEuPs2TAXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the MNIST images\n",
        "X_train_mnist_flat = X_train_mnist.reshape(X_train_mnist.shape[0], -1)\n",
        "\n",
        "# Normalize the data\n",
        "X_train_mnist_flat = X_train_mnist_flat / 255.0\n",
        "\n",
        "# Run KMeans\n",
        "kmeans_mnist = KMeans(n_clusters=10)\n",
        "kmeans_mnist.fit(X_train_mnist_flat)\n",
        "\n",
        "# Evaluate the KMeans objective\n",
        "def kmeans_objective(X, centroids, labels):\n",
        "    return np.sum([np.linalg.norm(X[labels == k] - centroids[k])**2 for k in range(centroids.shape[0])])\n",
        "\n",
        "objective_mnist = kmeans_objective(X_train_mnist_flat, kmeans_mnist.centroids, kmeans_mnist.labels)\n",
        "print(f\"KMeans Objective for MNIST (K=10): {objective_mnist}\")"
      ],
      "metadata": {
        "id": "-XM320fTSdTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(true_labels: np.ndarray, pred_labels: np.ndarray) -> tuple:\n",
        "  \"\"\"Entropy-based evaluation of a label assignment.\n",
        "\n",
        "  Parameters:\n",
        "    true_labels: the ground-truth class labels on the input data.\n",
        "    pred_labels: the predicted class labels on the input data.\n",
        "\n",
        "  Returns:\n",
        "    a tuple (CM, (cs_e, cr_e, we)) containing the confusion matrix `CM`, the class entropies `cs_e`,\n",
        "    the cluster entropies `cr_e`, and the averaged weighted entropies `we`.\n",
        "  \"\"\"\n",
        "  from scipy.stats import entropy\n",
        "\n",
        "  assert len(true_labels) == len(pred_labels), \"Label predictions don't match\"\n",
        "\n",
        "  ## Map the labels to index set {0, 1, ..., k - 1 }\n",
        "  t_classes, t_labels = np.unique(true_labels, return_inverse=True)\n",
        "  p_classes, p_labels = np.unique(pred_labels, return_inverse=True)\n",
        "  assert np.all(np.isin(p_classes, t_classes)), \"Predicted class outside of labels given\"\n",
        "\n",
        "  ## Accumulate the counts\n",
        "  n_classes = len(t_classes)\n",
        "  CM = np.zeros(shape=(n_classes, n_classes), dtype=np.uint32)\n",
        "  ind = np.ravel_multi_index([t_labels, p_labels], CM.shape)\n",
        "  np.add.at(CM.ravel(), ind, 1)\n",
        "\n",
        "  ## Compute the entropy of the empirical row/column distributions\n",
        "  empirical_dist = lambda x: x / np.sum(x)\n",
        "  cluster_entropy = np.apply_along_axis(lambda x: entropy(empirical_dist(x), base=2), 0, CM)\n",
        "  class_entropy = np.apply_along_axis(lambda x: entropy(empirical_dist(x), base=2), 1, CM)\n",
        "\n",
        "  ## Average w/ count weights\n",
        "  w_cluster_entropy = np.sum(cluster_entropy * CM.sum(axis=0)) / len(y)\n",
        "  w_class_entropy = np.sum(class_entropy * CM.sum(axis=1)) / len(y)\n",
        "  w_entropies = np.array([w_class_entropy, w_cluster_entropy])\n",
        "\n",
        "  with np.printoptions(precision=3):\n",
        "    print(f\"Class Entropies: {class_entropy}\")\n",
        "    print(f\"Cluster Entropies: {cluster_entropy}\")\n",
        "    print(f\"Weighted average entropies: {w_entropies}, (avg: {np.mean(w_entropies):.3f})\")\n",
        "  return CM, (w_class_entropy, w_cluster_entropy, w_entropies)"
      ],
      "metadata": {
        "id": "AeZ0zARFQIeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2**\n",
        "\n",
        "An auction house decides each morning, randomly based on internal rules, what class of products will be auctioned: Cars, Jewelry, Paintings, or Houses.\n",
        "Each class has it own bidders who are called to place bids on matching days, characterized by a bidding_rate parameter λ, and assumed that bidding intervals overall follows negative exponential distribution . That is, probability for a bid to not occur decreases exponentially with length of time. For each day we record the number of bids, which theory dictates must follow E[#bids] = λ, E[bidding_interval] = 1 / λ\n",
        "\n",
        "Part A (25 points). Given that exponential distribution assumption on bidding intervals, figure out the proper distribution for the #bids/day, parametrized by λ. You can use online resources to do so.\n",
        "\n",
        "Part B (75 points) The file contains counts of auction bids for 10000 days, without specifying which class was auctioned per day. Estimate the rate_bidding parameter for each class (4 λ values) and also estimate how many days each class was auctioned (4 counts out of 10000).\n",
        "Hint: use EM on a mixture of 4 distributions found in part A. You can use libraries for distribution computation (pdf), but the EM steps have to be your own implementation. Here is a possible result\n",
        "Estimated λ-s: [ 6.13 15.22 1.97 22.34]\n",
        "Estimated #days : [3087 1272 1953 3687 ]"
      ],
      "metadata": {
        "id": "V0G1lDVcQr_1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BVwNIHpXQx2y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}