{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPqGI04mNPDYuLGXcUEzKDk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1: Simple Sampling**\n",
        "\n",
        "You are not allowed to use sampling libraries/functions. But you can use rand() call to generate a pseudo-uniform value in [0,1]; you can also use a library that computes the pdf(x|params). make sure to recap first Rejection Sampling and Inverse Transform Sampling\n",
        "\n",
        "A. Implement simple sampling from continuous distributions: uniform (min, max, sample_size) and gaussian (mu, sigma, sample_size)\n",
        "\n",
        "B. Implement sampling from a 2-dim Gaussian Distribution (2d mu, 2d sigma, sample_size)\n",
        "\n",
        "C. Implement wihtout-replacement sampling from a discrete non-uniform distribution (given as input) following the Steven's method described in class ( paper ). Test it on desired sample sizes N significantly smaller than population size M (for example N=20 M=300)"
      ],
      "metadata": {
        "id": "43pzv3vwOKF_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "h4E6bFvBK9oW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d242044e-095c-49fe-fc16-e5314e9a689d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniform samples: [0.7412560056782214, 4.510359422572372, 1.7967491516657563, 6.497415231861189, 3.1559703932164576]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def sample_uniform(a, b, sample_size):\n",
        "    \"\"\"\n",
        "    Generates samples from a Uniform(a, b) distribution.\n",
        "\n",
        "    Args:\n",
        "        a (float): Lower bound.\n",
        "        b (float): Upper bound.\n",
        "        sample_size (int): Number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        list of float: Uniform samples.\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    for _ in range(sample_size):\n",
        "        u = random.random()  # rand() returns value in [0,1)\n",
        "        sample = a + (b - a) * u\n",
        "        samples.append(sample)\n",
        "    return samples\n",
        "\n",
        "# Example usage:\n",
        "print(\"Uniform samples:\", sample_uniform(0, 10, 5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def sample_gaussian(mu, sigma, sample_size):\n",
        "    \"\"\"\n",
        "    Generates samples from a Gaussian distribution N(mu, sigma^2)\n",
        "    using the Box-Muller transform.\n",
        "\n",
        "    Args:\n",
        "        mu (float): Mean.\n",
        "        sigma (float): Standard deviation.\n",
        "        sample_size (int): Number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        list of float: Gaussian samples.\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    # Process two samples per iteration using Box-Muller:\n",
        "    for _ in range(sample_size // 2):\n",
        "        u1 = random.random()\n",
        "        u2 = random.random()\n",
        "        # Avoid taking log of zero:\n",
        "        if u1 == 0:\n",
        "            u1 = 1e-10\n",
        "        z0 = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n",
        "        z1 = math.sqrt(-2 * math.log(u1)) * math.sin(2 * math.pi * u2)\n",
        "        samples.append(mu + sigma * z0)\n",
        "        samples.append(mu + sigma * z1)\n",
        "    # If sample_size is odd, generate one additional sample:\n",
        "    if sample_size % 2 != 0:\n",
        "        u1 = random.random()\n",
        "        u2 = random.random()\n",
        "        if u1 == 0:\n",
        "            u1 = 1e-10\n",
        "        z0 = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n",
        "        samples.append(mu + sigma * z0)\n",
        "    return samples\n",
        "\n",
        "# Example usage:\n",
        "print(\"Gaussian samples:\", sample_gaussian(0, 1, 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpUYVB9AURMn",
        "outputId": "ae623bde-4cd3-47c0-be80-392d35dacfab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian samples: [1.3147763751451838, 1.7580061073619908, 1.1759820215522332, -0.16384846967437985, 0.7893206211577134]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_gaussian_2d(mu_vector, sigma_vector, sample_size):\n",
        "    \"\"\"\n",
        "    Generates samples from a 2D Gaussian distribution with independent components.\n",
        "\n",
        "    Args:\n",
        "        mu_vector (list or tuple of floats): [mu_x, mu_y].\n",
        "        sigma_vector (list or tuple of floats): [sigma_x, sigma_y].\n",
        "        sample_size (int): Number of 2D samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        list of [float, float]: List of 2D sample points.\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "\n",
        "    # For each sample, generate 2 independent standard normal numbers:\n",
        "    for _ in range(sample_size):\n",
        "        # We can use the Box-Muller transform to generate a pair; here we only need 2 numbers.\n",
        "        u1 = random.random()\n",
        "        u2 = random.random()\n",
        "        if u1 == 0:\n",
        "            u1 = 1e-10\n",
        "        z1 = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n",
        "        # For the second normal, either call Box-Muller again or use the orthogonal value:\n",
        "        # For clarity, generate a new pair for each sample\n",
        "        u3 = random.random()\n",
        "        u4 = random.random()\n",
        "        if u3 == 0:\n",
        "            u3 = 1e-10\n",
        "        z2 = math.sqrt(-2 * math.log(u3)) * math.cos(2 * math.pi * u4)\n",
        "\n",
        "        # Transform to the desired mean and standard deviation:\n",
        "        sample_x = mu_vector[0] + sigma_vector[0] * z1\n",
        "        sample_y = mu_vector[1] + sigma_vector[1] * z2\n",
        "        samples.append([sample_x, sample_y])\n",
        "\n",
        "    return samples\n",
        "\n",
        "# Example usage:\n",
        "mu_2d = [0, 0]\n",
        "sigma_2d = [1, 1]\n",
        "print(\"2D Gaussian samples:\", sample_gaussian_2d(mu_2d, sigma_2d, 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJTdiSsWUWyW",
        "outputId": "770a0f8c-e4e7-4716-ccf8-0e853b69c856"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2D Gaussian samples: [[0.7206460801908975, -1.4689833765028348], [-0.6323498129811431, 0.694239523698937], [-0.08302570754409883, 0.473705499284006], [-0.32329636447046806, -0.41492666093887587], [-0.6533854539418171, -0.7224874317673764]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_sample_without_replacement(weights, sample_size):\n",
        "    \"\"\"\n",
        "    Samples indices without replacement from a list of weights using\n",
        "    the method based on exponential variates (often called Steven's method).\n",
        "\n",
        "    Args:\n",
        "        weights (list of floats): Non-negative weights for the population (length M).\n",
        "        sample_size (int): Number of items to sample (N), N << M.\n",
        "\n",
        "    Returns:\n",
        "        list of int: Selected indices (without replacement).\n",
        "    \"\"\"\n",
        "    keys = []\n",
        "    # Compute a key for every element.\n",
        "    for i, w in enumerate(weights):\n",
        "        # To avoid division by zero\n",
        "        if w <= 0:\n",
        "            key = float('inf')\n",
        "        else:\n",
        "            u = random.random() # Uniform in [0,1)\n",
        "            # To avoid log(0)\n",
        "            if u == 0:\n",
        "                u = 1e-10\n",
        "            key = -math.log(u) / w\n",
        "        keys.append((key, i))\n",
        "\n",
        "    # Sort items by their keys (ascending order) and select the sample_size smallest keys.\n",
        "    keys.sort(key=lambda x: x[0])\n",
        "    selected_indices = [index for (_, index) in keys[:sample_size]]\n",
        "    return selected_indices\n",
        "\n",
        "# Test scenario: N=20, M=300. Generate a synthetic weight vector.\n",
        "M = 300\n",
        "N = 20\n",
        "# For example, let the weights be random positive numbers\n",
        "population_weights = [random.random() + 0.1 for _ in range(M)]  # adding 0.1 to avoid zero weights\n",
        "\n",
        "selected = weighted_sample_without_replacement(population_weights, N)\n",
        "print(\"Selected indices (without replacement):\", selected)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz7e72YXUtYA",
        "outputId": "f8b148ef-2d79-4db9-a3a0-8d05ee7958f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected indices (without replacement): [98, 123, 80, 96, 264, 267, 109, 10, 268, 257, 215, 209, 249, 272, 203, 153, 125, 286, 95, 68]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2: Conditional Sampling**\n",
        "\n",
        "Implement Gibbs Sampling for a multidim gaussian generative joint, by using the conditionals which are also gaussian distributions . The minimum requirement is for joint to have D=2 variables and for Gibbs to alternate between the two."
      ],
      "metadata": {
        "id": "5-6pzvRXU9vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "def sample_gaussian_value(mu, sigma):\n",
        "    \"\"\"\n",
        "    Generate a single sample from a Gaussian distribution N(mu, sigma^2)\n",
        "    using the Box–Muller transform.\n",
        "    \"\"\"\n",
        "    u1 = random.random()\n",
        "    u2 = random.random()\n",
        "    # Prevent u1 from being 0\n",
        "    if u1 == 0:\n",
        "        u1 = 1e-10\n",
        "    z = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n",
        "    return mu + sigma * z\n",
        "\n",
        "def gibbs_sampling_2d(mu, sigma, rho, num_samples, burn_in=100):\n",
        "    \"\"\"\n",
        "    Perform Gibbs sampling for a 2-dimensional Gaussian distribution.\n",
        "\n",
        "    The joint distribution:\n",
        "         (X, Y) ~ N( [mu_x, mu_y], Σ )\n",
        "    where Σ is defined as:\n",
        "         [sigma_x^2        rho*sigma_x*sigma_y]\n",
        "         [rho*sigma_x*sigma_y   sigma_y^2     ]\n",
        "\n",
        "    The conditionals are:\n",
        "      X|Y=y ~ N(mu_x + rho*(sigma_x/sigma_y)*(y-mu_y), (1-rho^2)*sigma_x^2)\n",
        "      Y|X=x ~ N(mu_y + rho*(sigma_y/sigma_x)*(x-mu_x), (1-rho^2)*sigma_y^2)\n",
        "\n",
        "    Args:\n",
        "        mu: list or tuple of two floats [mu_x, mu_y]\n",
        "        sigma: list or tuple of two floats [sigma_x, sigma_y]\n",
        "        rho: float, correlation coefficient between X and Y, must be in (-1, 1)\n",
        "        num_samples: int, number of samples to collect (after burn-in)\n",
        "        burn_in: int, number of burn-in iterations (default is 100)\n",
        "\n",
        "    Returns:\n",
        "        List of [x, y] samples from the joint distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the chain. We can start at the mean.\n",
        "    x = mu[0]\n",
        "    y = mu[1]\n",
        "\n",
        "    samples = []\n",
        "\n",
        "    # Total iterations: burn_in + actual samples\n",
        "    total_iterations = burn_in + num_samples\n",
        "\n",
        "    for iteration in range(total_iterations):\n",
        "        # Sample X given current Y:\n",
        "        mean_x_cond = mu[0] + rho * (sigma[0]/sigma[1]) * (y - mu[1])\n",
        "        var_x_cond = (1 - rho**2) * sigma[0]**2\n",
        "        x = sample_gaussian_value(mean_x_cond, math.sqrt(var_x_cond))\n",
        "\n",
        "        # Sample Y given updated X:\n",
        "        mean_y_cond = mu[1] + rho * (sigma[1]/sigma[0]) * (x - mu[0])\n",
        "        var_y_cond = (1 - rho**2) * sigma[1]**2\n",
        "        y = sample_gaussian_value(mean_y_cond, math.sqrt(var_y_cond))\n",
        "\n",
        "        # After burn_in, collect the sample.\n",
        "        if iteration >= burn_in:\n",
        "            samples.append([x, y])\n",
        "\n",
        "    return samples\n",
        "\n",
        "# Example Usage\n",
        "\n",
        "# Parameters for the joint Gaussian:\n",
        "mu = [0.0, 0.0]\n",
        "sigma = [1.0, 1.0]  # Standard deviations for X and Y\n",
        "rho = 0.8           # Correlation coefficient\n",
        "\n",
        "# Number of samples we want to collect (after burn-in)\n",
        "num_samples = 1000\n",
        "burn_in = 200\n",
        "\n",
        "# Run Gibbs Sampling\n",
        "samples = gibbs_sampling_2d(mu, sigma, rho, num_samples, burn_in)\n",
        "\n",
        "# Print a few samples to check\n",
        "for i, sample in enumerate(samples[:10]):\n",
        "    print(f\"Sample {i+1}: X = {sample[0]:.4f}, Y = {sample[1]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuIFyzb-U_s9",
        "outputId": "e0ee08c6-d19a-4028-b0db-e33df23846ad"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1: X = -0.5984, Y = -0.7303\n",
            "Sample 2: X = -1.5374, Y = -0.9483\n",
            "Sample 3: X = -0.9707, Y = -0.1778\n",
            "Sample 4: X = 0.2747, Y = -0.7135\n",
            "Sample 5: X = -0.5919, Y = -1.1076\n",
            "Sample 6: X = -0.3659, Y = -0.2633\n",
            "Sample 7: X = -0.5607, Y = 0.4132\n",
            "Sample 8: X = 0.7554, Y = 0.6642\n",
            "Sample 9: X = 0.5580, Y = 0.0755\n",
            "Sample 10: X = -0.6485, Y = -0.1405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3: Implement your own baby-LDA**\n",
        "\n",
        "Implement your own LDA using Gibbs Sampling, following this paper and this easy-to-read book . Gibbs Sampling is a lot slower than EM alternatives, so this can take some time; use a smaller sample of docs/words at first.\n",
        "\n",
        "20NG train dataset 11280 docs x 53000 words\n",
        "Small sonnet dataset (one per line) 154 docs x 3092 words"
      ],
      "metadata": {
        "id": "p9U6pXmlbOf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Utility functions for cleaning and processing\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean raw text: lowercase, remove digits and punctuation, and split into words.\n",
        "    Only used for 20NG (which is raw).\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)                   # remove digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)                # remove punctuation\n",
        "    words = text.split()\n",
        "    words = [word for word in words if len(word) > 2]   # filter very short words\n",
        "    return words\n",
        "\n",
        "def build_vocab(docs, min_freq=5):\n",
        "    \"\"\"\n",
        "    Create vocabulary from a list of tokenized documents.\n",
        "\n",
        "    Args:\n",
        "      docs: list of documents (each document is a list of tokens)\n",
        "      min_freq: minimum frequency threshold for inclusion in vocabulary\n",
        "\n",
        "    Returns:\n",
        "      vocab: sorted list of words\n",
        "      word_to_index: dictionary mapping word to index\n",
        "    \"\"\"\n",
        "    counter = Counter()\n",
        "    for doc in docs:\n",
        "        counter.update(doc)\n",
        "    vocab = [word for word, count in counter.items() if count >= min_freq]\n",
        "    vocab = sorted(vocab)\n",
        "    word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "    return vocab, word_to_index\n",
        "\n",
        "def docs_to_indices(docs, word_to_index):\n",
        "    \"\"\"\n",
        "    Convert tokenized documents (list of words) to lists of indices.\n",
        "\n",
        "    Args:\n",
        "      docs: list of documents (each document is a list of tokens)\n",
        "      word_to_index: dictionary mapping word to index\n",
        "\n",
        "    Returns:\n",
        "      List of documents represented as lists of word indices.\n",
        "    \"\"\"\n",
        "    docs_indices = []\n",
        "    for doc in docs:\n",
        "        indices = [word_to_index[word] for word in doc if word in word_to_index]\n",
        "        if indices:\n",
        "            docs_indices.append(indices)\n",
        "    return docs_indices\n",
        "\n",
        "# Data-loading functions\n",
        "\n",
        "def load_20ng_dataset():\n",
        "    \"\"\"\n",
        "    Load 20NG training data using scikit-learn. The texts are raw so we clean them.\n",
        "\n",
        "    Returns:\n",
        "      List of documents, where each document is a list of words.\n",
        "    \"\"\"\n",
        "    # Remove headers, footers, and quotes to get cleaner documents.\n",
        "    newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "    docs = []\n",
        "    for text in newsgroups.data:\n",
        "        words = clean_text(text)\n",
        "        if words:\n",
        "            docs.append(words)\n",
        "    return docs\n",
        "\n",
        "def load_sonnets_preprocessed(filename):\n",
        "    \"\"\"\n",
        "    Load a preprocessed sonnets dataset where each line is a document\n",
        "    and tokens are already pre-tokenized (separated by whitespace).\n",
        "\n",
        "    Args:\n",
        "      filename: Path to the preprocessed sonnets file.\n",
        "\n",
        "    Returns:\n",
        "      List of documents, each a list of tokens.\n",
        "    \"\"\"\n",
        "    docs = []\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                tokens = line.strip().split()\n",
        "                if tokens:\n",
        "                    docs.append(tokens)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {filename}: {e}\")\n",
        "    return docs\n",
        "\n",
        "# Baby LDA implementation using Gibbs Sampling\n",
        "\n",
        "def discrete_sample(probabilities):\n",
        "    \"\"\"\n",
        "    Sample an index from a discrete distribution with unnormalized probabilities.\n",
        "    \"\"\"\n",
        "    total = sum(probabilities)\n",
        "    r = random.uniform(0, total)\n",
        "    cumulative = 0.0\n",
        "    for i, p in enumerate(probabilities):\n",
        "        cumulative += p\n",
        "        if cumulative >= r:\n",
        "            return i\n",
        "    return len(probabilities) - 1\n",
        "\n",
        "def baby_lda(DOCS, Vocab, K, alpha, beta, iterations):\n",
        "    \"\"\"\n",
        "    Run baby LDA using Gibbs sampling.\n",
        "\n",
        "    Args:\n",
        "      DOCS: List of documents (each a list of word indices).\n",
        "      Vocab: List of strings; the vocabulary.\n",
        "      K: int; number of topics.\n",
        "      alpha: float; Dirichlet prior for doc-topic distributions.\n",
        "      beta: float; Dirichlet prior for topic-word distributions.\n",
        "      iterations: int; number of Gibbs sampling iterations.\n",
        "\n",
        "    Returns:\n",
        "      Z: Topic assignments for each word (list of lists).\n",
        "      A: Document-topic count matrix (N x K).\n",
        "      B: Topic-word count matrix (K x W).\n",
        "    \"\"\"\n",
        "    N = len(DOCS) # number of documents\n",
        "    W = len(Vocab) # vocabulary size\n",
        "\n",
        "    # Initialize A with alpha for each topic per document.\n",
        "    A = [[alpha for _ in range(K)] for _ in range(N)]\n",
        "    # Initialize B with beta for each word per topic.\n",
        "    B = [[beta for _ in range(W)] for _ in range(K)]\n",
        "    # BSUM holds the sum over words for each topic.\n",
        "    BSUM = [beta * W for _ in range(K)]\n",
        "    # Z holds topic assignments for each word in each document, initialized to -1.\n",
        "    Z = [[-1 for _ in doc] for doc in DOCS]\n",
        "\n",
        "    # Gibbs sampling iterations.\n",
        "    for it in range(iterations):\n",
        "        for d, doc in enumerate(DOCS):\n",
        "            for i, w in enumerate(doc):\n",
        "                current_topic = Z[d][i]\n",
        "                # Remove current assignment if it exists.\n",
        "                if current_topic != -1:\n",
        "                    A[d][current_topic] -= 1\n",
        "                    B[current_topic][w] -= 1\n",
        "                    BSUM[current_topic] -= 1\n",
        "\n",
        "                # Calculate unnormalized probabilities for each topic.\n",
        "                topic_probs = []\n",
        "                for k in range(K):\n",
        "                    prob = A[d][k] * (B[k][w] / BSUM[k])\n",
        "                    topic_probs.append(prob)\n",
        "\n",
        "                new_topic = discrete_sample(topic_probs)\n",
        "                Z[d][i] = new_topic\n",
        "                # Update counts with new assignment.\n",
        "                A[d][new_topic] += 1\n",
        "                B[new_topic][w] += 1\n",
        "                BSUM[new_topic] += 1\n",
        "\n",
        "        if (it + 1) % 10 == 0 or it == 0:\n",
        "            print(f\"Iteration {it+1} of {iterations} completed.\")\n",
        "    return Z, A, B\n",
        "\n",
        "def print_top_words_per_topic(B, Vocab, top_n=10):\n",
        "    \"\"\"\n",
        "    Display the top N words for each topic based on topic-word counts.\n",
        "    \"\"\"\n",
        "    for k, counts in enumerate(B):\n",
        "        word_count_pairs = [(Vocab[w], count) for w, count in enumerate(counts)]\n",
        "        word_count_pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_words = [word for word, count in word_count_pairs[:top_n]]\n",
        "        print(f\"Topic {k+1}: {', '.join(top_words)}\")"
      ],
      "metadata": {
        "id": "F-ZfouzifUaw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Run baby LDA on 20NG train data and sonnetsPreprocessed.txt\n",
        "    # LDA Parameters\n",
        "    K = 6                 # Number of topics\n",
        "    alpha = 5.0           # Prior for document-topic distributions\n",
        "    beta = 2.0            # Prior for topic-word distributions\n",
        "    iterations = 100      # Number of Gibbs sampling iterations\n",
        "\n",
        "    # 20NG Dataset\n",
        "    print(\"Loading 20NG train dataset\")\n",
        "    docs_20ng = load_20ng_dataset()\n",
        "    print(f\"Loaded {len(docs_20ng)} documents from 20NG\")\n",
        "    # Build vocabulary; adjust min_freq as needed (e.g., 10)\n",
        "    vocab_20ng, word_to_index_20ng = build_vocab(docs_20ng, min_freq=10)\n",
        "    print(f\"20NG Vocabulary size: {len(vocab_20ng)}\")\n",
        "    docs_20ng_indices = docs_to_indices(docs_20ng, word_to_index_20ng)\n",
        "\n",
        "    print(\"\\nRunning baby LDA on 20NG dataset\")\n",
        "    Z_20ng, A_20ng, B_20ng = baby_lda(docs_20ng_indices, vocab_20ng, K, alpha, beta, iterations)\n",
        "    print(\"\\nTop words per topic for 20NG:\")\n",
        "    print_top_words_per_topic(B_20ng, vocab_20ng)\n",
        "\n",
        "    # Sonnets Dataset\n",
        "    print(\"\\nLoading preprocessed sonnets dataset (sonnetsPreprocessed.txt)\")\n",
        "    sonnet_file = \"sonnetsPreprocessed.txt\"\n",
        "    docs_sonnet = load_sonnets_preprocessed(sonnet_file)\n",
        "    print(f\"Loaded {len(docs_sonnet)} sonnet documents\")\n",
        "    # For sonnets the file is already preprocessed, so no additional cleaning is done.\n",
        "    # Use a lower min_freq since the dataset is small.\n",
        "    vocab_sonnet, word_to_index_sonnet = build_vocab(docs_sonnet, min_freq=1)\n",
        "    print(f\"Sonnets Vocabulary size: {len(vocab_sonnet)}\")\n",
        "    docs_sonnet_indices = docs_to_indices(docs_sonnet, word_to_index_sonnet)\n",
        "\n",
        "    print(\"\\nRunning baby LDA on sonnets dataset\")\n",
        "    Z_sonnet, A_sonnet, B_sonnet = baby_lda(docs_sonnet_indices, vocab_sonnet, K, alpha, beta, iterations)\n",
        "    print(\"\\nTop words per topic for sonnets:\")\n",
        "    print_top_words_per_topic(B_sonnet, vocab_sonnet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaWl-38hsF-S",
        "outputId": "c5fdd852-7cc3-4776-c6e2-baa41ae63e76"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 20NG train dataset...\n",
            "Loaded 11001 documents from 20NG.\n",
            "20NG Vocabulary size: 11228\n",
            "\n",
            "Running baby LDA on 20NG dataset...\n",
            "Iteration 1 of 100 completed.\n",
            "Iteration 10 of 100 completed.\n",
            "Iteration 20 of 100 completed.\n",
            "Iteration 30 of 100 completed.\n",
            "Iteration 40 of 100 completed.\n",
            "Iteration 50 of 100 completed.\n",
            "Iteration 60 of 100 completed.\n",
            "Iteration 70 of 100 completed.\n",
            "Iteration 80 of 100 completed.\n",
            "Iteration 90 of 100 completed.\n",
            "Iteration 100 of 100 completed.\n",
            "\n",
            "Top words per topic for 20NG:\n",
            "Topic 1: the, and, was, that, they, were, had, from, with, there\n",
            "Topic 2: the, that, and, not, you, are, this, have, but, for\n",
            "Topic 3: the, and, for, maxaxaxaxaxaxaxaxaxaxaxaxaxaxax, with, this, you, are, can, from\n",
            "Topic 4: the, and, for, this, are, will, that, with, from, has\n",
            "Topic 5: the, and, for, was, with, will, but, new, year, game\n",
            "Topic 6: you, the, have, that, and, for, but, this, would, dont\n",
            "\n",
            "Loading preprocessed sonnets dataset (sonnetsPreprocessed.txt)...\n",
            "Loaded 154 sonnet documents.\n",
            "Sonnets Vocabulary size: 3092\n",
            "\n",
            "Running baby LDA on sonnets dataset...\n",
            "Iteration 1 of 100 completed.\n",
            "Iteration 10 of 100 completed.\n",
            "Iteration 20 of 100 completed.\n",
            "Iteration 30 of 100 completed.\n",
            "Iteration 40 of 100 completed.\n",
            "Iteration 50 of 100 completed.\n",
            "Iteration 60 of 100 completed.\n",
            "Iteration 70 of 100 completed.\n",
            "Iteration 80 of 100 completed.\n",
            "Iteration 90 of 100 completed.\n",
            "Iteration 100 of 100 completed.\n",
            "\n",
            "Top words per topic for sonnets:\n",
            "Topic 1: far, whose, hath, mistress, muse, myself, nor, breast, once, own\n",
            "Topic 2: thought, o, old, nor, times, every, none, show, give, keep\n",
            "Topic 3: sweet, age, deeds, love, worth, art, away, come, dead, fire\n",
            "Topic 4: thy, thou, love, thee, doth, mine, shall, eyes, heart, beauty\n",
            "Topic 5: make, nothing, hand, tell, time, true, age, doth, form, heaven\n",
            "Topic 6: sight, days, thou, against, day, death, die, hours, poor, better\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For 20NG, I am unhapppy about the filler words and the weird 'maxaxaxa' output I got, will try to re run again."
      ],
      "metadata": {
        "id": "FkKIHkrX7TZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = {\n",
        "    \"the\", \"and\", \"for\", \"was\", \"but\", \"with\", \"are\", \"have\", \"this\", \"that\",\n",
        "    \"from\", \"will\", \"not\", \"you\", \"its\", \"they\", \"their\", \"etc\",\n",
        "    \"maxaxaxaxaxaxaxaxaxaxaxaxaxaxax\"  # Known artifact\n",
        "}\n",
        "\n",
        "# Refined cleaning function for 20NG data\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean raw text: lowercase, remove digits/punctuation, split into words,\n",
        "    then filter out words that are too short, non-alphabetic or in STOPWORDS.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    words = text.split()\n",
        "    # Filter words: longer than 2, alphabetic only, not in our stopwords.\n",
        "    words = [word for word in words if len(word) > 2 and word.isalpha() and word not in STOPWORDS]\n",
        "    return words"
      ],
      "metadata": {
        "id": "A2BFh_yw7cX2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # LDA Parameters\n",
        "    K = 6       # Number of topics\n",
        "    alpha = 5.0\n",
        "    beta = 2.0\n",
        "    iterations = 100\n",
        "\n",
        "    print(\"Loading refined 20NG train dataset\")\n",
        "    docs_20ng = load_20ng_dataset()\n",
        "    print(f\"Loaded {len(docs_20ng)} documents from 20NG\")\n",
        "    vocab, word_to_index = build_vocab(docs_20ng, min_freq=10)\n",
        "    print(f\"20NG Vocabulary size after cleaning: {len(vocab)}\")\n",
        "    docs_20ng_indices = docs_to_indices(docs_20ng, word_to_index)\n",
        "\n",
        "    print(\"\\nRunning baby LDA on refined 20NG dataset\")\n",
        "    Z, A, B = baby_lda(docs_20ng_indices, vocab, K, alpha, beta, iterations)\n",
        "    print(\"\\nTop words per topic after cleaning:\")\n",
        "    print_top_words_per_topic(B, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-joD_dEo-bK5",
        "outputId": "c5decd8f-2752-4123-9646-18c311f6b1af"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading refined 20NG train dataset...\n",
            "Loaded 10999 documents from 20NG.\n",
            "20NG Vocabulary size after cleaning: 11173\n",
            "\n",
            "Running baby LDA on refined 20NG dataset...\n",
            "Iteration 1 of 100 completed.\n",
            "Iteration 10 of 100 completed.\n",
            "Iteration 20 of 100 completed.\n",
            "Iteration 30 of 100 completed.\n",
            "Iteration 40 of 100 completed.\n",
            "Iteration 50 of 100 completed.\n",
            "Iteration 60 of 100 completed.\n",
            "Iteration 70 of 100 completed.\n",
            "Iteration 80 of 100 completed.\n",
            "Iteration 90 of 100 completed.\n",
            "Iteration 100 of 100 completed.\n",
            "\n",
            "Top words per topic after cleaning:\n",
            "Topic 1: dont, there, about, just, what, out, know, would, were, like\n",
            "Topic 2: can, your, use, file, any, which, windows, get, drive, also\n",
            "Topic 3: game, year, team, has, new, good, his, games, first, play\n",
            "Topic 4: who, what, god, his, all, people, one, there, your, which\n",
            "Topic 5: space, can, than, has, more, which, one, also, some, other\n",
            "Topic 6: has, any, what, which, can, would, government, other, key, been\n"
          ]
        }
      ]
    }
  ]
}