{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOq3QDaiSMWUPdl1OgGK+b+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HalgasAdrian/CS5230-Coursework/blob/main/HW5B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1: Simple Sampling**\n",
        "\n",
        "You are not allowed to use sampling libraries/functions. But you can use rand() call to generate a pseudo-uniform value in [0,1]; you can also use a library that computes the pdf(x|params). make sure to recap first Rejection Sampling and Inverse Transform Sampling\n",
        "\n",
        "A. Implement simple sampling from continuous distributions: uniform (min, max, sample_size) and gaussian (mu, sigma, sample_size)\n",
        "\n",
        "B. Implement sampling from a 2-dim Gaussian Distribution (2d mu, 2d sigma, sample_size)\n",
        "\n",
        "C. Implement wihtout-replacement sampling from a discrete non-uniform distribution (given as input) following the Steven's method described in class ( paper ). Test it on desired sample sizes N significantly smaller than population size M (for example N=20 M=300)"
      ],
      "metadata": {
        "id": "43pzv3vwOKF_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h4E6bFvBK9oW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21404ea2-3e07-409d-aff4-78ecd110ebbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniform samples: [6.987538055619814, 2.6119309767933196, 9.453894979763586, 9.007527242424382, 2.542066158250824]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def sample_uniform(a, b, sample_size):\n",
        "    \"\"\"\n",
        "    Generates samples from a Uniform(a, b) distribution.\n",
        "\n",
        "    Args:\n",
        "        a (float): Lower bound.\n",
        "        b (float): Upper bound.\n",
        "        sample_size (int): Number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        list of float: Uniform samples.\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    for _ in range(sample_size):\n",
        "        u = random.random()  # rand() returns value in [0,1)\n",
        "        sample = a + (b - a) * u\n",
        "        samples.append(sample)\n",
        "    return samples\n",
        "\n",
        "# Example usage:\n",
        "print(\"Uniform samples:\", sample_uniform(0, 10, 5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def sample_gaussian(mu, sigma, sample_size):\n",
        "    \"\"\"\n",
        "    Generates samples from a Gaussian distribution N(mu, sigma^2)\n",
        "    using the Box-Muller transform.\n",
        "\n",
        "    Args:\n",
        "        mu (float): Mean.\n",
        "        sigma (float): Standard deviation.\n",
        "        sample_size (int): Number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        list of float: Gaussian samples.\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    # Process two samples per iteration using Box-Muller:\n",
        "    for _ in range(sample_size // 2):\n",
        "        u1 = random.random()\n",
        "        u2 = random.random()\n",
        "        # Avoid taking log of zero:\n",
        "        if u1 == 0:\n",
        "            u1 = 1e-10\n",
        "        z0 = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n",
        "        z1 = math.sqrt(-2 * math.log(u1)) * math.sin(2 * math.pi * u2)\n",
        "        samples.append(mu + sigma * z0)\n",
        "        samples.append(mu + sigma * z1)\n",
        "    # If sample_size is odd, generate one additional sample:\n",
        "    if sample_size % 2 != 0:\n",
        "        u1 = random.random()\n",
        "        u2 = random.random()\n",
        "        if u1 == 0:\n",
        "            u1 = 1e-10\n",
        "        z0 = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n",
        "        samples.append(mu + sigma * z0)\n",
        "    return samples\n",
        "\n",
        "# Example usage:\n",
        "print(\"Gaussian samples:\", sample_gaussian(0, 1, 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpUYVB9AURMn",
        "outputId": "a1c2d934-823a-4747-807a-66efc63a56a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian samples: [-1.2823551358522043, -0.3916149557147224, 2.0250496247006513, -1.0520974229953688, -0.5949501315390834]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_gaussian_2d(mu_vector, sigma_vector, sample_size):\n",
        "    \"\"\"\n",
        "    Generates samples from a 2D Gaussian distribution with independent components.\n",
        "\n",
        "    Args:\n",
        "        mu_vector (list or tuple of floats): [mu_x, mu_y].\n",
        "        sigma_vector (list or tuple of floats): [sigma_x, sigma_y].\n",
        "        sample_size (int): Number of 2D samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        list of [float, float]: List of 2D sample points.\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "\n",
        "    # For each sample, generate 2 independent standard normal numbers:\n",
        "    for _ in range(sample_size):\n",
        "        # We can use the Box-Muller transform to generate a pair; here we only need 2 numbers.\n",
        "        u1 = random.random()\n",
        "        u2 = random.random()\n",
        "        if u1 == 0:\n",
        "            u1 = 1e-10\n",
        "        z1 = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n",
        "        # For the second normal, either call Box-Muller again or use the orthogonal value:\n",
        "        # For clarity, generate a new pair for each sample\n",
        "        u3 = random.random()\n",
        "        u4 = random.random()\n",
        "        if u3 == 0:\n",
        "            u3 = 1e-10\n",
        "        z2 = math.sqrt(-2 * math.log(u3)) * math.cos(2 * math.pi * u4)\n",
        "\n",
        "        # Transform to the desired mean and standard deviation:\n",
        "        sample_x = mu_vector[0] + sigma_vector[0] * z1\n",
        "        sample_y = mu_vector[1] + sigma_vector[1] * z2\n",
        "        samples.append([sample_x, sample_y])\n",
        "\n",
        "    return samples\n",
        "\n",
        "# Example usage:\n",
        "mu_2d = [0, 0]\n",
        "sigma_2d = [1, 1]\n",
        "print(\"2D Gaussian samples:\", sample_gaussian_2d(mu_2d, sigma_2d, 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJTdiSsWUWyW",
        "outputId": "e5360d5a-3493-4365-f642-48512a7dca93"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2D Gaussian samples: [[-0.962700359874964, -0.6487905857268136], [-0.8560819786230847, -0.34007792571958795], [0.9278813267525409, 1.4630738821801952], [-0.5644163262626185, -0.8470550936827791], [0.5868602180560467, 1.3332663475644468]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_sample_without_replacement(weights, sample_size):\n",
        "    \"\"\"\n",
        "    Samples indices without replacement from a list of weights using\n",
        "    the method based on exponential variates (often called Steven's method).\n",
        "\n",
        "    Args:\n",
        "        weights (list of floats): Non-negative weights for the population (length M).\n",
        "        sample_size (int): Number of items to sample (N), N << M.\n",
        "\n",
        "    Returns:\n",
        "        list of int: Selected indices (without replacement).\n",
        "    \"\"\"\n",
        "    keys = []\n",
        "    # Compute a key for every element.\n",
        "    for i, w in enumerate(weights):\n",
        "        # To avoid division by zero, handle zero-weight items appropriately.\n",
        "        # Here we simply assign an infinite key so they will not be selected.\n",
        "        if w <= 0:\n",
        "            key = float('inf')\n",
        "        else:\n",
        "            u = random.random()  # Uniform in [0,1)\n",
        "            # To avoid log(0) in the extremely unlikely event u==0:\n",
        "            if u == 0:\n",
        "                u = 1e-10\n",
        "            key = -math.log(u) / w\n",
        "        keys.append((key, i))\n",
        "\n",
        "    # Sort items by their keys (ascending order) and select the sample_size smallest keys.\n",
        "    keys.sort(key=lambda x: x[0])\n",
        "    selected_indices = [index for (_, index) in keys[:sample_size]]\n",
        "    return selected_indices\n",
        "\n",
        "# Test scenario: N=20, M=300. We generate a synthetic weight vector.\n",
        "M = 300\n",
        "N = 20\n",
        "# For example, let the weights be random positive numbers (not necessarily summing to 1)\n",
        "population_weights = [random.random() + 0.1 for _ in range(M)]  # adding 0.1 to avoid zero weights\n",
        "\n",
        "selected = weighted_sample_without_replacement(population_weights, N)\n",
        "print(\"Selected indices (without replacement):\", selected)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz7e72YXUtYA",
        "outputId": "26020e27-d487-4860-9c46-2fa5563a7d3d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected indices (without replacement): [8, 176, 62, 224, 77, 177, 18, 13, 58, 136, 74, 86, 0, 111, 126, 263, 46, 145, 44, 188]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2: Conditional Sampling**\n",
        "\n",
        "Implement Gibbs Sampling for a multidim gaussian generative joint, by using the conditionals which are also gaussian distributions . The minimum requirement is for joint to have D=2 variables and for Gibbs to alternate between the two."
      ],
      "metadata": {
        "id": "5-6pzvRXU9vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "def sample_gaussian_value(mu, sigma):\n",
        "    \"\"\"\n",
        "    Generate a single sample from a Gaussian distribution N(mu, sigma^2)\n",
        "    using the Box–Muller transform.\n",
        "    \"\"\"\n",
        "    u1 = random.random()\n",
        "    u2 = random.random()\n",
        "    # Guard against zero for u1 (extremely unlikely)\n",
        "    if u1 == 0:\n",
        "        u1 = 1e-10\n",
        "    z = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n",
        "    return mu + sigma * z\n",
        "\n",
        "def gibbs_sampling_2d(mu, sigma, rho, num_samples, burn_in=100):\n",
        "    \"\"\"\n",
        "    Perform Gibbs sampling for a 2-dimensional Gaussian distribution.\n",
        "\n",
        "    The joint distribution:\n",
        "         (X, Y) ~ N( [mu_x, mu_y], Σ )\n",
        "    where Σ is defined as:\n",
        "         [sigma_x^2        rho*sigma_x*sigma_y]\n",
        "         [rho*sigma_x*sigma_y   sigma_y^2     ]\n",
        "\n",
        "    The conditionals are:\n",
        "      X|Y=y ~ N(mu_x + rho*(sigma_x/sigma_y)*(y-mu_y), (1-rho^2)*sigma_x^2)\n",
        "      Y|X=x ~ N(mu_y + rho*(sigma_y/sigma_x)*(x-mu_x), (1-rho^2)*sigma_y^2)\n",
        "\n",
        "    Args:\n",
        "        mu: list or tuple of two floats [mu_x, mu_y]\n",
        "        sigma: list or tuple of two floats [sigma_x, sigma_y]\n",
        "        rho: float, correlation coefficient between X and Y, must be in (-1, 1)\n",
        "        num_samples: int, number of samples to collect (after burn-in)\n",
        "        burn_in: int, number of burn-in iterations (default is 100)\n",
        "\n",
        "    Returns:\n",
        "        List of [x, y] samples from the joint distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the chain. We can start at the mean.\n",
        "    x = mu[0]\n",
        "    y = mu[1]\n",
        "\n",
        "    samples = []\n",
        "\n",
        "    # Total iterations: burn_in + actual samples\n",
        "    total_iterations = burn_in + num_samples\n",
        "\n",
        "    for iteration in range(total_iterations):\n",
        "        # Sample X given current Y:\n",
        "        mean_x_cond = mu[0] + rho * (sigma[0]/sigma[1]) * (y - mu[1])\n",
        "        var_x_cond = (1 - rho**2) * sigma[0]**2\n",
        "        x = sample_gaussian_value(mean_x_cond, math.sqrt(var_x_cond))\n",
        "\n",
        "        # Sample Y given updated X:\n",
        "        mean_y_cond = mu[1] + rho * (sigma[1]/sigma[0]) * (x - mu[0])\n",
        "        var_y_cond = (1 - rho**2) * sigma[1]**2\n",
        "        y = sample_gaussian_value(mean_y_cond, math.sqrt(var_y_cond))\n",
        "\n",
        "        # After burn_in, collect the sample.\n",
        "        if iteration >= burn_in:\n",
        "            samples.append([x, y])\n",
        "\n",
        "    return samples\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "# Parameters for the joint Gaussian:\n",
        "mu = [0.0, 0.0]\n",
        "sigma = [1.0, 1.0]  # Standard deviations for X and Y\n",
        "rho = 0.8           # Correlation coefficient\n",
        "\n",
        "# Number of samples we want to collect (after burn-in)\n",
        "num_samples = 1000\n",
        "burn_in = 200\n",
        "\n",
        "# Run Gibbs Sampling\n",
        "samples = gibbs_sampling_2d(mu, sigma, rho, num_samples, burn_in)\n",
        "\n",
        "# Print a few samples to check\n",
        "for i, sample in enumerate(samples[:10]):\n",
        "    print(f\"Sample {i+1}: X = {sample[0]:.4f}, Y = {sample[1]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuIFyzb-U_s9",
        "outputId": "2553aaf9-9d18-4962-b1ab-0fa0af5f4516"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1: X = 0.6713, Y = 0.5571\n",
            "Sample 2: X = 0.1021, Y = 1.5488\n",
            "Sample 3: X = 0.8961, Y = 0.3792\n",
            "Sample 4: X = -0.2160, Y = 0.2510\n",
            "Sample 5: X = -0.9930, Y = 0.7896\n",
            "Sample 6: X = 1.0555, Y = 0.2000\n",
            "Sample 7: X = -0.0766, Y = 0.5402\n",
            "Sample 8: X = 0.0592, Y = 0.6796\n",
            "Sample 9: X = 0.4553, Y = 0.7211\n",
            "Sample 10: X = 0.1736, Y = -0.7669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3: Implement your own baby-LDA**\n",
        "\n",
        "Implement your own LDA using Gibbs Sampling, following this paper and this easy-to-read book . Gibbs Sampling is a lot slower than EM alternatives, so this can take some time; use a smaller sample of docs/words at first.\n",
        "\n",
        "20NG train dataset 11280 docs x 53000 words\n",
        "Small sonnet dataset (one per line) 154 docs x 3092 words"
      ],
      "metadata": {
        "id": "p9U6pXmlbOf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# --- For 20NG: Import from scikit-learn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Utility functions for cleaning and processing\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean raw text: lowercase, remove digits and punctuation, and split into words.\n",
        "    Only used for 20NG (which is raw).\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)                   # remove digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)                # remove punctuation\n",
        "    words = text.split()\n",
        "    words = [word for word in words if len(word) > 2]   # filter very short words\n",
        "    return words\n",
        "\n",
        "def build_vocab(docs, min_freq=5):\n",
        "    \"\"\"\n",
        "    Create vocabulary from a list of tokenized documents.\n",
        "\n",
        "    Args:\n",
        "      docs: list of documents (each document is a list of tokens)\n",
        "      min_freq: minimum frequency threshold for inclusion in vocabulary\n",
        "\n",
        "    Returns:\n",
        "      vocab: sorted list of words\n",
        "      word_to_index: dictionary mapping word to index\n",
        "    \"\"\"\n",
        "    counter = Counter()\n",
        "    for doc in docs:\n",
        "        counter.update(doc)\n",
        "    vocab = [word for word, count in counter.items() if count >= min_freq]\n",
        "    vocab = sorted(vocab)\n",
        "    word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "    return vocab, word_to_index\n",
        "\n",
        "def docs_to_indices(docs, word_to_index):\n",
        "    \"\"\"\n",
        "    Convert tokenized documents (list of words) to lists of indices.\n",
        "\n",
        "    Args:\n",
        "      docs: list of documents (each document is a list of tokens)\n",
        "      word_to_index: dictionary mapping word to index\n",
        "\n",
        "    Returns:\n",
        "      List of documents represented as lists of word indices.\n",
        "    \"\"\"\n",
        "    docs_indices = []\n",
        "    for doc in docs:\n",
        "        indices = [word_to_index[word] for word in doc if word in word_to_index]\n",
        "        if indices:\n",
        "            docs_indices.append(indices)\n",
        "    return docs_indices\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Data-loading functions\n",
        "\n",
        "def load_20ng_dataset():\n",
        "    \"\"\"\n",
        "    Load 20NG training data using scikit-learn. The texts are raw so we clean them.\n",
        "\n",
        "    Returns:\n",
        "      List of documents, where each document is a list of words.\n",
        "    \"\"\"\n",
        "    # Remove headers, footers, and quotes to get cleaner documents.\n",
        "    newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "    docs = []\n",
        "    for text in newsgroups.data:\n",
        "        words = clean_text(text)\n",
        "        if words:\n",
        "            docs.append(words)\n",
        "    return docs\n",
        "\n",
        "def load_sonnets_preprocessed(filename):\n",
        "    \"\"\"\n",
        "    Load a preprocessed sonnets dataset where each line is a document\n",
        "    and tokens are already pre-tokenized (separated by whitespace).\n",
        "\n",
        "    Args:\n",
        "      filename: Path to the preprocessed sonnets file.\n",
        "\n",
        "    Returns:\n",
        "      List of documents, each a list of tokens.\n",
        "    \"\"\"\n",
        "    docs = []\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                # Since the file is preprocessed, assume tokens are space-separated.\n",
        "                tokens = line.strip().split()\n",
        "                if tokens:\n",
        "                    docs.append(tokens)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {filename}: {e}\")\n",
        "    return docs\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Baby LDA implementation using Gibbs Sampling\n",
        "\n",
        "def discrete_sample(probabilities):\n",
        "    \"\"\"\n",
        "    Sample an index from a discrete distribution with unnormalized probabilities.\n",
        "    \"\"\"\n",
        "    total = sum(probabilities)\n",
        "    r = random.uniform(0, total)\n",
        "    cumulative = 0.0\n",
        "    for i, p in enumerate(probabilities):\n",
        "        cumulative += p\n",
        "        if cumulative >= r:\n",
        "            return i\n",
        "    return len(probabilities) - 1  # fallback\n",
        "\n",
        "def baby_lda(DOCS, Vocab, K, alpha, beta, iterations):\n",
        "    \"\"\"\n",
        "    Run baby LDA using Gibbs sampling.\n",
        "\n",
        "    Args:\n",
        "      DOCS: List of documents (each a list of word indices).\n",
        "      Vocab: List of strings; the vocabulary.\n",
        "      K: int; number of topics.\n",
        "      alpha: float; Dirichlet prior for doc-topic distributions.\n",
        "      beta: float; Dirichlet prior for topic-word distributions.\n",
        "      iterations: int; number of Gibbs sampling iterations.\n",
        "\n",
        "    Returns:\n",
        "      Z: Topic assignments for each word (list of lists).\n",
        "      A: Document-topic count matrix (N x K).\n",
        "      B: Topic-word count matrix (K x W).\n",
        "    \"\"\"\n",
        "    N = len(DOCS)         # number of documents\n",
        "    W = len(Vocab)        # vocabulary size\n",
        "\n",
        "    # Initialize A with alpha for each topic per document.\n",
        "    A = [[alpha for _ in range(K)] for _ in range(N)]\n",
        "    # Initialize B with beta for each word per topic.\n",
        "    B = [[beta for _ in range(W)] for _ in range(K)]\n",
        "    # BSUM holds the sum over words for each topic.\n",
        "    BSUM = [beta * W for _ in range(K)]\n",
        "    # Z holds topic assignments for each word in each document, initialized to -1.\n",
        "    Z = [[-1 for _ in doc] for doc in DOCS]\n",
        "\n",
        "    # Gibbs sampling iterations.\n",
        "    for it in range(iterations):\n",
        "        for d, doc in enumerate(DOCS):\n",
        "            for i, w in enumerate(doc):\n",
        "                current_topic = Z[d][i]\n",
        "                # Remove current assignment if it exists.\n",
        "                if current_topic != -1:\n",
        "                    A[d][current_topic] -= 1\n",
        "                    B[current_topic][w] -= 1\n",
        "                    BSUM[current_topic] -= 1\n",
        "\n",
        "                # Calculate unnormalized probabilities for each topic.\n",
        "                topic_probs = []\n",
        "                for k in range(K):\n",
        "                    prob = A[d][k] * (B[k][w] / BSUM[k])\n",
        "                    topic_probs.append(prob)\n",
        "\n",
        "                new_topic = discrete_sample(topic_probs)\n",
        "                Z[d][i] = new_topic\n",
        "                # Update counts with new assignment.\n",
        "                A[d][new_topic] += 1\n",
        "                B[new_topic][w] += 1\n",
        "                BSUM[new_topic] += 1\n",
        "\n",
        "        if (it + 1) % 10 == 0 or it == 0:\n",
        "            print(f\"Iteration {it+1} of {iterations} completed.\")\n",
        "    return Z, A, B\n",
        "\n",
        "def print_top_words_per_topic(B, Vocab, top_n=10):\n",
        "    \"\"\"\n",
        "    Display the top N words for each topic based on topic-word counts.\n",
        "    \"\"\"\n",
        "    for k, counts in enumerate(B):\n",
        "        word_count_pairs = [(Vocab[w], count) for w, count in enumerate(counts)]\n",
        "        word_count_pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_words = [word for word, count in word_count_pairs[:top_n]]\n",
        "        print(f\"Topic {k+1}: {', '.join(top_words)}\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Main execution: Run baby LDA on 20NG train data and sonnetsPreprocessed.txt\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- LDA Parameters\n",
        "    K = 6                 # Number of topics\n",
        "    alpha = 5.0           # Prior for document-topic distributions\n",
        "    beta = 2.0            # Prior for topic-word distributions\n",
        "    iterations = 100      # Number of Gibbs sampling iterations\n",
        "\n",
        "    # --- 20NG Dataset\n",
        "    print(\"Loading 20NG train dataset...\")\n",
        "    docs_20ng = load_20ng_dataset()\n",
        "    print(f\"Loaded {len(docs_20ng)} documents from 20NG.\")\n",
        "    # Build vocabulary; adjust min_freq as needed (e.g., 10)\n",
        "    vocab_20ng, word_to_index_20ng = build_vocab(docs_20ng, min_freq=10)\n",
        "    print(f\"20NG Vocabulary size: {len(vocab_20ng)}\")\n",
        "    docs_20ng_indices = docs_to_indices(docs_20ng, word_to_index_20ng)\n",
        "\n",
        "    print(\"\\nRunning baby LDA on 20NG dataset...\")\n",
        "    Z_20ng, A_20ng, B_20ng = baby_lda(docs_20ng_indices, vocab_20ng, K, alpha, beta, iterations)\n",
        "    print(\"\\nTop words per topic for 20NG:\")\n",
        "    print_top_words_per_topic(B_20ng, vocab_20ng)\n",
        "\n",
        "    # --- Sonnets Dataset\n",
        "    print(\"\\nLoading preprocessed sonnets dataset (sonnetsPreprocessed.txt)...\")\n",
        "    sonnet_file = \"sonnetsPreprocessed.txt\"   # Ensure this file path is correct\n",
        "    docs_sonnet = load_sonnets_preprocessed(sonnet_file)\n",
        "    print(f\"Loaded {len(docs_sonnet)} sonnet documents.\")\n",
        "    # For sonnets the file is already preprocessed, so no additional cleaning is done.\n",
        "    # Use a lower min_freq since the dataset is small.\n",
        "    vocab_sonnet, word_to_index_sonnet = build_vocab(docs_sonnet, min_freq=1)\n",
        "    print(f\"Sonnets Vocabulary size: {len(vocab_sonnet)}\")\n",
        "    docs_sonnet_indices = docs_to_indices(docs_sonnet, word_to_index_sonnet)\n",
        "\n",
        "    print(\"\\nRunning baby LDA on sonnets dataset...\")\n",
        "    Z_sonnet, A_sonnet, B_sonnet = baby_lda(docs_sonnet_indices, vocab_sonnet, K, alpha, beta, iterations)\n",
        "    print(\"\\nTop words per topic for sonnets:\")\n",
        "    print_top_words_per_topic(B_sonnet, vocab_sonnet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "F-ZfouzifUaw",
        "outputId": "1171a27d-d43b-43f2-819d-cbe8ff2e5242"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 20NG train dataset...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4c9f560f2df9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;31m# --- 20NG Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading 20NG train dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mdocs_20ng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_20ng_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded {len(docs_20ng)} documents from 20NG.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Build vocabulary; adjust min_freq as needed (e.g., 10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-4c9f560f2df9>\u001b[0m in \u001b[0;36mload_20ng_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# Remove headers, footers, and quotes to get cleaner documents.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mnewsgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'headers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'footers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quotes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnewsgroups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_twenty_newsgroups.py\u001b[0m in \u001b[0;36mfetch_20newsgroups\u001b[0;34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y, n_retries, delay)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload_if_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading 20news dataset. This may take a few minutes.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             cache = _download_20newsgroups(\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mtarget_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtwenty_home\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mcache_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_twenty_newsgroups.py\u001b[0m in \u001b[0;36m_download_20newsgroups\u001b[0;34m(target_dir, cache_path, n_retries, delay)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Decompressing %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r:gz\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mtarfile_extractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36mtarfile_extractall\u001b[0;34m(tarfile, path)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtarfile_extractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/tarfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, numeric_owner, filter)\u001b[0m\n\u001b[1;32m   2292\u001b[0m                 \u001b[0;31m# extracting contents can reset mtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2293\u001b[0m                 \u001b[0mdirectories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2294\u001b[0;31m             self._extract_one(tarinfo, path, set_attrs=not tarinfo.isdir(),\n\u001b[0m\u001b[1;32m   2295\u001b[0m                               numeric_owner=numeric_owner)\n\u001b[1;32m   2296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/tarfile.py\u001b[0m in \u001b[0;36m_extract_one\u001b[0;34m(self, tarinfo, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2357\u001b[0;31m             self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n\u001b[0m\u001b[1;32m   2358\u001b[0m                                  \u001b[0mset_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m                                  numeric_owner=numeric_owner)\n",
            "\u001b[0;32m/usr/lib/python3.11/tarfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2423\u001b[0m         \u001b[0;31m# forward slashes to platform specific separators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2424\u001b[0m         \u001b[0mtargetpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2425\u001b[0;31m         \u001b[0mtargetpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2427\u001b[0m         \u001b[0;31m# Create all upper directories.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}