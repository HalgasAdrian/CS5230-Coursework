{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNPD5suB/DGJoTYITxXF2ME"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1: Recommender System using Collaborative Filtering**\n",
        "\n",
        "Implement a Movie Recommendation System and run it on the Movie Lens Dataset (Train vs Test). Mesure performance on test set using RMSE\n",
        "\n",
        "\n",
        "\n",
        "*   First you are required to compute first a user-user similarity based on ratings and movies in common\n",
        "*   Second, make rating predictions on the test set following the KNN idea: a prediction (user, movie) is the weighted average of other users' rating for the movie, weighted by user-similarity to the given user."
      ],
      "metadata": {
        "id": "anhmU0COXwct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSRJYASjVOzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb14466-edb5-40d8-c469-2fdc511783d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE (user-user CF, k=5): 2.4288\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load Movielens 100k data\n",
        "# Download and extract the dataset, then point to the u.data file\n",
        "with zipfile.ZipFile('ml-100k.zip','r') as z:\n",
        "    with z.open('ml-100k/u.data') as f:\n",
        "        ratings = pd.read_csv(\n",
        "            f,\n",
        "            sep='\\t',\n",
        "            names=['user_id','item_id','rating','timestamp'],\n",
        "            usecols=['user_id','item_id','rating']\n",
        "        )\n",
        "\n",
        "# 2. Train/test split\n",
        "train_df, test_df = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Build user-item rating matrix from train set\n",
        "ratings_matrix = train_df.pivot_table(\n",
        "    index='user_id', columns='item_id', values='rating'\n",
        ")\n",
        "\n",
        "# 4. Compute user-user similarity (Pearson)\n",
        "# We use pandas' corr on transposed matrix: correlations over items axis\n",
        "sim_matrix = ratings_matrix.T.corr(method='pearson', min_periods=1)\n",
        "\n",
        "# 5. Prediction function using KNN\n",
        "\n",
        "def predict_rating(user_id, item_id, ratings_mat, sim_mat, k=5):\n",
        "    if item_id not in ratings_mat.columns:\n",
        "        # Movie not seen in training\n",
        "        return ratings_mat.stack().mean()\n",
        "\n",
        "    # similarities for target user to all others\n",
        "    sims = sim_mat[user_id].drop(index=user_id).dropna()\n",
        "    # ratings of other users for this item\n",
        "    item_ratings = ratings_mat[item_id].dropna()\n",
        "\n",
        "    # intersect neighbors\n",
        "    common_users = sims.index.intersection(item_ratings.index)\n",
        "    if len(common_users) == 0:\n",
        "        return ratings_mat.stack().mean()\n",
        "\n",
        "    # select top-k similar users\n",
        "    top_k = sims.loc[common_users].abs().nlargest(k).index\n",
        "    top_sims = sims.loc[top_k]\n",
        "    top_ratings = item_ratings.loc[top_k]\n",
        "\n",
        "    # weighted average\n",
        "    num = (top_sims * top_ratings).sum()\n",
        "    den = top_sims.abs().sum()\n",
        "    if den == 0:\n",
        "        return ratings_mat.stack().mean()\n",
        "    return num / den\n",
        "\n",
        "# 6. Predict for test set and evaluate RMSE\n",
        "preds = []\n",
        "truths = []\n",
        "for _, row in test_df.iterrows():\n",
        "    u, i, r = row['user_id'], row['item_id'], row['rating']\n",
        "    pred = predict_rating(u, i, ratings_matrix, sim_matrix, k=5)\n",
        "    preds.append(pred)\n",
        "    truths.append(r)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(truths, preds))\n",
        "print(f\"Test RMSE (user-user CF, k=5): {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3A: Social Community Detection**\n",
        "\n",
        "Implement edge-removal community detection algorithm on the Flicker Graph. Use the betweeness idea on edges and the Girvan–Newman Algorithm. The original dataset graph has more than 5M edges; in DM_resources there are 4 different sub-sampled graphs with edge counts from 2K to 600K; you can use these if the original is too big.\n",
        "You should use a library to support graph operations (edges, vertices, paths, degrees, etc). We used igraph in python which also have builtin community detection algorithms (not allowed); these are useful as a way to evaluate communities you obtain"
      ],
      "metadata": {
        "id": "z6mZswjbYnYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install igraph’s C core\n",
        "!apt-get update -qq\n",
        "!apt-get install -y libigraph0-dev\n",
        "\n",
        "# install the Python bindings\n",
        "!pip install python-igraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ut6dJtQYumf",
        "outputId": "fbeeed0b-3810-4b4c-a388-643faf448147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package libigraph0-dev is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "\n",
            "E: Package 'libigraph0-dev' has no installation candidate\n",
            "Requirement already satisfied: python-igraph in /usr/local/lib/python3.11/dist-packages (0.11.8)\n",
            "Requirement already satisfied: igraph==0.11.8 in /usr/local/lib/python3.11/dist-packages (from python-igraph) (0.11.8)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from igraph==0.11.8->python-igraph) (1.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gJYxrhAWk8xK",
        "outputId": "b4eddbe8-1115-4334-c1a1-a0fb447f8f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "This module defines load_graph() and girvan_newman(); import and call them interactively.",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m This module defines load_graph() and girvan_newman(); import and call them interactively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3B: Social Community Detection**\n",
        "\n",
        "Implement the modularity detection algorithm on this artificial graph (adj matrix written in sparse format : each row is an edge [node_id, node_id, 1]). You will need to compute the modularity matrix B and its highest-val eigenvector V1. The split vector S (+1 / -1) aligns by sign with V1; follow this paper. Partition the graph in two parts K=2).\n",
        "Optional: Partition the graph in more than 2 parts, try to figure out what is a \"natural\" K here."
      ],
      "metadata": {
        "id": "KkdSlJf9YvCX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OOsFiP3cY1l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 4: Knowledge Base Question Answering**\n",
        "\n",
        "Given is knowledge graph with entities and relations, questions with starting entity and answers, and their word embedding . For each question, navigate the graph from the start entity outwards until you find appropriate answer entities.\n",
        "Utils functions (similarity, load_graphs) are given, but you can choose not to use them. This python file contains the helper functions for this homework, the only update needed to use this file is to fill in the file paths.\n",
        "\n",
        "- The number of correct answers varies (could be 1, could many), use F1 to compare your answers with the given correct answers\n",
        "- Utils functions (similarity, load_graphs) are given, but you can choose not to use them.\n",
        "- Answers are given to be used for evaluation only, DO NOT USE ANSWERS IN YOUR GRAPH TRAVERSAL.\n",
        "Your strategy should be a graph traversal augmented with scoring of paths; you might discard paths not promising along the way. This is similar to a focused crawl strategy. You will take a query (question) that you are trying to answer, it will have a starting entity. Begin your traversal at that starting entity, and look at all adjacent edges. Use get_rel_score_word2vecbase to get a similarity score for each edge, and traverse the edges that are promising. This part is up to you, you can cut off scores below a certain threshold, or take only the top percentage, or weight it based on the average.\n",
        "\n",
        "There are many valid strategies. You will continue to traverse a path, until the score starts to decrease, or you notice the similarity score drops significantly (compared to the previous edges). Overall try a few different approaches, and choose one that gives you the best overall F1 score."
      ],
      "metadata": {
        "id": "gR94tZjQY20p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script implements a beam‐search–based question‐answering pipeline over a knowledge graph. First, it loads a pretrained Word2Vec model, the graph edges, and the question annotations. For each question, it starts from the given entity, scores each outgoing relation by its cosine similarity to the question text, and expands only the top paths up to a fixed beam width and depth while avoiding cycles. Once the search terminates, it collects the final nodes reached and compares them against the gold answers to compute per‐query and overall F₁ scores. Helper functions provide detailed debugging output, and the entire evaluation runs automatically when the script is executed."
      ],
      "metadata": {
        "id": "pIJq8aMB7VH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install numpy==1.26.4\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEa5EdNzdq_g",
        "outputId": "9912b192-f6b5-44a1-f6bb-742c10febd55"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "from gensim import models\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# File locations\n",
        "W2V_PATH      = '../content/word2vec_train_dev.dat.txt'\n",
        "GRAPH_PATH    = '../content/graph.txt'\n",
        "ANNOT_PATH    = '../content/annotations.txt'\n",
        "\n",
        "# Beam-search parameters - MODIFIED FOR DEBUGGING\n",
        "BEAM_WIDTH  = 10     # INCREASED: how many partial paths to keep at each depth\n",
        "MAX_DEPTH   = 4      # INCREASED: maximum number of hops to follow\n",
        "SCORE_THRESH = 0.001  # LOWERED: discard any edge with similarity below this\n",
        "\n",
        "# Function to get the cosine similarity between a relation and query\n",
        "word2vec_model = models.Word2Vec.load(W2V_PATH)\n",
        "\n",
        "def get_rel_score_word2vecbase(rel: str, query: str) -> float:\n",
        "    \"\"\"\n",
        "    Get score for query and relation. Used to inform exploration of knowledge graph.\n",
        "\n",
        "    :param rel: relation, or edge in knowledge graph\n",
        "    :param query: query, question to answer\n",
        "    :return: float score similarity between question and relation\n",
        "    \"\"\"\n",
        "    # Add ns: prefix if not already present\n",
        "    rel = 'ns:' + rel if not rel.startswith('ns:') else rel\n",
        "\n",
        "    # Check if relation is in vocabulary\n",
        "    if rel not in word2vec_model.wv:\n",
        "        return 0.0\n",
        "\n",
        "    # Simple tokenization (fall back to basic splitting if NLTK has issues)\n",
        "    try:\n",
        "        words = word_tokenize(query.lower())\n",
        "    except:\n",
        "        words = query.lower().split()\n",
        "\n",
        "    w_embs = []\n",
        "    for w in words:\n",
        "        if w in word2vec_model.wv:\n",
        "            w_embs.append(word2vec_model.wv[w])\n",
        "\n",
        "    # Handle empty embeddings case\n",
        "    if not w_embs:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate similarity\n",
        "    try:\n",
        "        similarities = cosine_similarity(w_embs, [word2vec_model.wv[rel]])\n",
        "        return np.mean(similarities)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def load_node_label_lookup(filepath: str) -> dict:\n",
        "    \"\"\"\n",
        "    Load the lookup dictionary for nodes from the provided json file.\n",
        "    \"\"\"\n",
        "    with open(filepath, 'rb') as fp:\n",
        "        return json.load(fp)\n",
        "\n",
        "\n",
        "def load_query_df(filepath: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load a simplified dataframe of queries.\n",
        "    \"\"\"\n",
        "    return pd.read_parquet(filepath)\n",
        "\n",
        "\n",
        "def load_graph() -> dict:\n",
        "    \"\"\"\n",
        "    Load the graph from the given file.\n",
        "    \"\"\"\n",
        "    graph = defaultdict(list)\n",
        "    for line in open(GRAPH_PATH):\n",
        "        try:\n",
        "            line = eval(line.strip())\n",
        "            graph[line[0]].append([line[1], line[2]])\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing line: {line}\")\n",
        "            print(f\"Error: {e}\")\n",
        "    return graph\n",
        "\n",
        "\n",
        "def load_queries() -> list:\n",
        "    \"\"\"\n",
        "    Load the original queries file.\n",
        "    \"\"\"\n",
        "    queries = []\n",
        "    for line in open(ANNOT_PATH):\n",
        "        try:\n",
        "            line = eval(line.strip())\n",
        "            queries.append(line)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing query line: {line}\")\n",
        "            print(f\"Error: {e}\")\n",
        "    return queries\n",
        "\n",
        "\n",
        "def answer_query(start: str,\n",
        "                 query: str,\n",
        "                 graph: dict,\n",
        "                 beam_width: int = BEAM_WIDTH,\n",
        "                 max_depth: int = MAX_DEPTH,\n",
        "                 score_thresh: float = SCORE_THRESH,\n",
        "                 debug: bool = False\n",
        "                ) -> set:\n",
        "    \"\"\"\n",
        "    Beam-search out from `start`. At each hop:\n",
        "      - score each outgoing edge by similarity to `query`\n",
        "      - prune those below `score_thresh`\n",
        "      - keep only top `beam_width` partial paths\n",
        "    Returns the set of final node IDs reached.\n",
        "    \"\"\"\n",
        "    if debug:\n",
        "        print(f\"\\nStarting beam search from node {start} for query: {query}\")\n",
        "\n",
        "    beams = [(start, [start], [])]\n",
        "    final_nodes = set()\n",
        "\n",
        "    for depth in range(max_depth):\n",
        "        if debug:\n",
        "            print(f\"\\nDepth {depth+1}:\")\n",
        "\n",
        "        new_beams = []\n",
        "        for node, path, scores in beams:\n",
        "            if node not in graph:\n",
        "                if debug:\n",
        "                    print(f\"  Node {node} has no outgoing edges, keeping as final\")\n",
        "                final_nodes.add(node)\n",
        "                continue\n",
        "\n",
        "            expansions = 0\n",
        "            for rel, nbr in graph.get(node, []):\n",
        "                if nbr in path:\n",
        "                    continue\n",
        "\n",
        "                s = get_rel_score_word2vecbase(rel, query)\n",
        "                if s < score_thresh:\n",
        "                    continue\n",
        "\n",
        "                new_beams.append((nbr, path + [nbr], scores + [s]))\n",
        "                expansions += 1\n",
        "\n",
        "                if debug and expansions <= 3:\n",
        "                    print(f\"  {node} -[{rel} ({s:.4f})]-> {nbr}\")\n",
        "\n",
        "            if expansions == 0 and len(scores) > 0:\n",
        "                if debug:\n",
        "                    print(f\"  No valid expansions from {node}, keeping as final\")\n",
        "                final_nodes.add(node)\n",
        "\n",
        "        if not new_beams:\n",
        "            if debug:\n",
        "                print(\"  No more valid paths, stopping search\")\n",
        "            break\n",
        "\n",
        "        new_beams.sort(key=lambda x: np.mean(x[2]), reverse=True)\n",
        "        beams = new_beams[:beam_width]\n",
        "\n",
        "        if debug:\n",
        "            print(f\"  Top {min(3, len(beams))} beams:\")\n",
        "            for i, (node, path, scores) in enumerate(beams[:3]):\n",
        "                mean_score = np.mean(scores) if scores else 0\n",
        "                print(f\"    {i+1}. Path: {' -> '.join(path[-3:])}... (score: {mean_score:.4f})\")\n",
        "\n",
        "    for node, _, scores in beams:\n",
        "        if scores:\n",
        "            final_nodes.add(node)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"\\nFinal predictions ({len(final_nodes)}):\")\n",
        "        for node in list(final_nodes)[:5]:\n",
        "            print(f\"  - {node}\")\n",
        "\n",
        "    return final_nodes\n",
        "\n",
        "\n",
        "def evaluate_all(graph: dict, queries: list, debug_limit: int = 3) -> float:\n",
        "    \"\"\"\n",
        "    Evaluates all queries and returns the overall F1 score.\n",
        "    \"\"\"\n",
        "    y_true, y_pred = [], []\n",
        "    per_query_results = []\n",
        "\n",
        "    for i, query in enumerate(queries):\n",
        "        idx, question, start_id, _, _, answers = query\n",
        "        gold_ids = {a['AnswerArgument'] for a in answers}\n",
        "\n",
        "        debug_mode = i < debug_limit\n",
        "        if debug_mode:\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(f\"DEBUGGING QUERY #{idx}: {question}\")\n",
        "            print(f\"Start node: {start_id}\")\n",
        "            print(f\"Number of expected answers: {len(gold_ids)}\")\n",
        "\n",
        "            print(\"Sample expected answers:\")\n",
        "            for ans_id in list(gold_ids)[:3]:\n",
        "                print(f\"  - {ans_id}\")\n",
        "\n",
        "        pred_ids = answer_query(start_id, question, graph, debug=debug_mode)\n",
        "\n",
        "        labels = list(gold_ids | pred_ids)\n",
        "        if not labels:\n",
        "            per_query_f1 = 1.0\n",
        "        else:\n",
        "            y_t = [1 if lbl in gold_ids else 0 for lbl in labels]\n",
        "            y_p = [1 if lbl in pred_ids else 0 for lbl in labels]\n",
        "            per_query_f1 = f1_score(y_t, y_p) if sum(y_p) > 0 else 0.0\n",
        "\n",
        "        per_query_results.append((idx, per_query_f1, len(gold_ids), len(pred_ids)))\n",
        "\n",
        "        for query_labels, is_gold, is_pred in zip(\n",
        "            labels,\n",
        "            [1 if lbl in gold_ids else 0 for lbl in labels],\n",
        "            [1 if lbl in pred_ids else 0 for lbl in labels]\n",
        "        ):\n",
        "            y_true.append(is_gold)\n",
        "            y_pred.append(is_pred)\n",
        "\n",
        "        if debug_mode:\n",
        "            print(f\"\\nPredictions ({len(pred_ids)}):\")\n",
        "            for pred in list(pred_ids)[:5]:\n",
        "                print(f\"  - {pred}\")\n",
        "\n",
        "            correct = gold_ids & pred_ids\n",
        "            missed = gold_ids - pred_ids\n",
        "            extra = pred_ids - gold_ids\n",
        "\n",
        "            print(f\"\\nResults:\")\n",
        "            print(f\"  Correct predictions: {len(correct)}\")\n",
        "            print(f\"  Missed answers: {len(missed)}\")\n",
        "            print(f\"  Extra predictions: {len(extra)}\")\n",
        "            print(f\"  F1 score: {per_query_f1:.4f}\")\n",
        "\n",
        "    overall_f1 = f1_score(y_true, y_pred) if sum(y_pred) > 0 else 0.0\n",
        "\n",
        "    print(\"\\nPer-query performance:\")\n",
        "    print(f\"{'ID':>5} {'F1 Score':>10} {'Gold Count':>12} {'Pred Count':>12}\")\n",
        "    print('-' * 45)\n",
        "    for idx, f1, gold_count, pred_count in per_query_results[:10]:\n",
        "        print(f\"{idx:>5} {f1:>10.4f} {gold_count:>12} {pred_count:>12}\")\n",
        "\n",
        "    f1_scores = [f1 for _, f1, _, _ in per_query_results]\n",
        "    print(f\"\\nF1 Score statistics:\")\n",
        "    print(f\"  Mean: {np.mean(f1_scores):.4f}\")\n",
        "    print(f\"  Median: {np.median(f1_scores):.4f}\")\n",
        "    print(f\"  Min: {np.min(f1_scores):.4f}\")\n",
        "    print(f\"  Max: {np.max(f1_scores):.4f}\")\n",
        "    print(f\"  Queries with F1 > 0: {sum(1 for f1 in f1_scores if f1 > 0)}/{len(f1_scores)}\")\n",
        "\n",
        "    return overall_f1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"Loading graph...\")\n",
        "    graph = load_graph()\n",
        "    print(f\"Graph loaded: {len(graph)} nodes\")\n",
        "\n",
        "    edge_count = sum(len(edges) for edges in graph.values())\n",
        "    print(f\"Total edges: {edge_count}\")\n",
        "    print(f\"Average edges per node: {edge_count/len(graph):.2f}\")\n",
        "\n",
        "    print(\"\\nLoading queries...\")\n",
        "    queries = load_queries()\n",
        "    print(f\"Queries loaded: {len(queries)}\")\n",
        "\n",
        "    print(\"\\nTesting word2vec model...\")\n",
        "    sample_words = ['who', 'what', 'when', 'where', 'ns:type']\n",
        "    for word in sample_words:\n",
        "        if word in word2vec_model.wv:\n",
        "            print(f\"'{word}' is in vocabulary\")\n",
        "        else:\n",
        "            print(f\"'{word}' is NOT in vocabulary\")\n",
        "\n",
        "    print(f\"\\nRunning beam-search with width={BEAM_WIDTH}, depth={MAX_DEPTH}, thresh={SCORE_THRESH}\")\n",
        "    f1 = evaluate_all(graph, queries)\n",
        "    print(f\"\\nOverall F1 score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "bMJvXy-XY-eA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7651bf1-5a2f-4210-db0c-5dc5312ee8c3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading graph...\n",
            "Graph loaded: 286 nodes\n",
            "Total edges: 1761\n",
            "Average edges per node: 6.16\n",
            "\n",
            "Loading queries...\n",
            "Queries loaded: 56\n",
            "\n",
            "Testing word2vec model...\n",
            "'who' is in vocabulary\n",
            "'what' is in vocabulary\n",
            "'when' is in vocabulary\n",
            "'where' is in vocabulary\n",
            "'ns:type' is NOT in vocabulary\n",
            "\n",
            "Running beam-search with width=10, depth=4, thresh=0.001\n",
            "\n",
            "================================================================================\n",
            "DEBUGGING QUERY #1: what time zones are there in the us\n",
            "Start node: m.09c7w0\n",
            "Number of expected answers: 9\n",
            "Sample expected answers:\n",
            "  - m.02hcv8\n",
            "  - m.02lcrv\n",
            "  - m.027wj2_\n",
            "\n",
            "Starting beam search from node m.09c7w0 for query: what time zones are there in the us\n",
            "\n",
            "Depth 1:\n",
            "  m.09c7w0 -[location.statistical_region.religions (0.2507)]-> m.03q1lwl\n",
            "  m.09c7w0 -[location.statistical_region.religions (0.2507)]-> m.03q1lvq\n",
            "  m.09c7w0 -[location.location.contains (0.2199)]-> m.0vzm\n",
            "  Top 3 beams:\n",
            "    1. Path: m.09c7w0 -> m.027wj2_... (score: 0.3495)\n",
            "    2. Path: m.09c7w0 -> m.02fqwt... (score: 0.3495)\n",
            "    3. Path: m.09c7w0 -> m.042g7t... (score: 0.3495)\n",
            "\n",
            "Depth 2:\n",
            "  No valid expansions from m.027wj2_, keeping as final\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.021czc\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.07b_l\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.01_d4\n",
            "  m.042g7t -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.059g4\n",
            "  m.02lcqs -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.059g4\n",
            "  m.02hczc -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.059g4\n",
            "  m.02hczc -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.07b_l\n",
            "  m.02lcrv -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.059g4\n",
            "  No valid expansions from m.027wjl3, keeping as final\n",
            "  m.02lctm -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.059g4\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.021czc\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.07b_l\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.01_d4\n",
            "  m.02lcrv -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.059g4\n",
            "  Top 3 beams:\n",
            "    1. Path: m.09c7w0 -> m.02fqwt -> m.021czc... (score: 0.3240)\n",
            "    2. Path: m.09c7w0 -> m.02fqwt -> m.07b_l... (score: 0.3240)\n",
            "    3. Path: m.09c7w0 -> m.02fqwt -> m.01_d4... (score: 0.3240)\n",
            "\n",
            "Depth 3:\n",
            "  m.021czc -[location.location.containedby (0.1975)]-> m.01_d4\n",
            "  m.021czc -[location.location.containedby (0.1975)]-> m.03v0t\n",
            "  m.07b_l -[location.location.time_zones (0.3495)]-> m.02hczc\n",
            "  m.07b_l -[location.us_state.capital (0.2941)]-> m.0vzm\n",
            "  m.07b_l -[location.location.time_zones (0.3495)]-> m.02hczc\n",
            "  m.01_d4 -[location.citytown.postal_codes (0.2690)]-> m.07nqmhw\n",
            "  m.01_d4 -[location.citytown.postal_codes (0.2690)]-> m.07nqmhf\n",
            "  m.01_d4 -[location.citytown.postal_codes (0.2690)]-> m.01_6pxw\n",
            "  m.03v0t -[location.location.contains (0.2199)]-> m.02jw0z\n",
            "  m.03v0t -[location.location.contains (0.2199)]-> m.01_d4\n",
            "  m.03v0t -[location.location.contains (0.2199)]-> m.01_6pxw\n",
            "  m.059g4 -[location.location.time_zones (0.3495)]-> m.042g7t\n",
            "  m.059g4 -[location.location.time_zones (0.3495)]-> m.02lcqs\n",
            "  m.059g4 -[location.location.time_zones (0.3495)]-> m.02hczc\n",
            "  m.01dlzc -[location.location.containedby (0.1975)]-> m.01_d4\n",
            "  m.01dlzc -[location.location.containedby (0.1975)]-> m.03v0t\n",
            "  m.0vzm -[base.biblioness.bibs_location.state (0.2907)]-> m.07b_l\n",
            "  m.0vzm -[location.location.containedby (0.1975)]-> m.07b_l\n",
            "  m.059g4 -[location.location.time_zones (0.3495)]-> m.02lcqs\n",
            "  m.059g4 -[location.location.time_zones (0.3495)]-> m.02hczc\n",
            "  m.059g4 -[location.location.events (0.1935)]-> m.09r3f\n",
            "  m.059g4 -[location.location.time_zones (0.3495)]-> m.042g7t\n",
            "  m.059g4 -[location.location.time_zones (0.3495)]-> m.02hczc\n",
            "  m.059g4 -[location.location.events (0.1935)]-> m.09r3f\n",
            "  m.059g4 -[location.location.time_zones (0.3495)]-> m.042g7t\n",
            "  m.059g4 -[location.location.time_zones (0.3495)]-> m.02lcqs\n",
            "  m.059g4 -[location.location.events (0.1935)]-> m.09r3f\n",
            "  Top 3 beams:\n",
            "    1. Path: m.02fqwt -> m.07b_l -> m.02hczc... (score: 0.3325)\n",
            "    2. Path: m.02fqwt -> m.07b_l -> m.02hczc... (score: 0.3325)\n",
            "    3. Path: m.02fqwt -> m.059g4 -> m.042g7t... (score: 0.3325)\n",
            "\n",
            "Depth 4:\n",
            "  m.02hczc -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.059g4\n",
            "  m.02hczc -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.059g4\n",
            "  No valid expansions from m.042g7t, keeping as final\n",
            "  No valid expansions from m.02lcqs, keeping as final\n",
            "  m.02hczc -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.07b_l\n",
            "  m.02hcv8 -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.0fv_t\n",
            "  m.02hcv8 -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.06yxd\n",
            "  m.02hcv8 -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.0mmyl\n",
            "  No valid expansions from m.02lctm, keeping as final\n",
            "  No valid expansions from m.02lcrv, keeping as final\n",
            "  No valid expansions from m.02lcqs, keeping as final\n",
            "  m.02hczc -[time.time_zone.locations_in_this_time_zone (0.2984)]-> m.07b_l\n",
            "  Top 3 beams:\n",
            "    1. Path: m.07b_l -> m.02hczc -> m.059g4... (score: 0.3240)\n",
            "    2. Path: m.07b_l -> m.02hczc -> m.059g4... (score: 0.3240)\n",
            "    3. Path: m.059g4 -> m.02hczc -> m.07b_l... (score: 0.3240)\n",
            "\n",
            "Final predictions (11):\n",
            "  - m.06yxd\n",
            "  - m.0fv_t\n",
            "  - m.0mmyl\n",
            "  - m.02lcrv\n",
            "  - m.027wj2_\n",
            "\n",
            "Predictions (11):\n",
            "  - m.06yxd\n",
            "  - m.0fv_t\n",
            "  - m.0mmyl\n",
            "  - m.02lcrv\n",
            "  - m.027wj2_\n",
            "\n",
            "Results:\n",
            "  Correct predictions: 6\n",
            "  Missed answers: 3\n",
            "  Extra predictions: 5\n",
            "  F1 score: 0.6000\n",
            "\n",
            "================================================================================\n",
            "DEBUGGING QUERY #2: what are major exports of the usa\n",
            "Start node: m.09c7w0\n",
            "Number of expected answers: 4\n",
            "Sample expected answers:\n",
            "  - m.03qtd_n\n",
            "  - m.015smg\n",
            "  - m.03q9wp2\n",
            "\n",
            "Starting beam search from node m.09c7w0 for query: what are major exports of the usa\n",
            "\n",
            "Depth 1:\n",
            "  m.09c7w0 -[location.statistical_region.religions (0.2946)]-> m.03q1lwl\n",
            "  m.09c7w0 -[location.statistical_region.religions (0.2946)]-> m.03q1lvq\n",
            "  m.09c7w0 -[location.location.contains (0.2249)]-> m.0vzm\n",
            "  Top 3 beams:\n",
            "    1. Path: m.09c7w0 -> m.04g4s8k... (score: 0.3086)\n",
            "    2. Path: m.09c7w0 -> m.04g4s90... (score: 0.3086)\n",
            "    3. Path: m.09c7w0 -> m.04g4s8q... (score: 0.3086)\n",
            "\n",
            "Depth 2:\n",
            "  m.04g4s8k -[location.imports_exports_by_industry.industry (0.3083)]-> m.03qtd_n\n",
            "  m.04g4s8k -[location.imports_exports_by_industry.industry (0.3083)]-> m.03qtd_n\n",
            "  m.04g4s8k -[location.imports_exports_by_industry.industry (0.3083)]-> m.03qtf10\n",
            "  m.04g4s90 -[location.imports_exports_by_industry.industry (0.3083)]-> m.03qtf10\n",
            "  m.04g4s90 -[location.imports_exports_by_industry.industry (0.3083)]-> m.015smg\n",
            "  m.04g4s90 -[location.imports_exports_by_industry.industry (0.3083)]-> m.03q9wp2\n",
            "  m.04g4s8q -[location.imports_exports_by_industry.industry (0.3083)]-> m.03qtf10\n",
            "  m.04g4s8q -[location.imports_exports_by_industry.industry (0.3083)]-> m.03qtd_n\n",
            "  m.04g4s8q -[location.imports_exports_by_industry.industry (0.3083)]-> m.03q9wp2\n",
            "  m.04g4s8k -[location.imports_exports_by_industry.industry (0.3083)]-> m.03qtd_n\n",
            "  m.04g4s8k -[location.imports_exports_by_industry.industry (0.3083)]-> m.03qtd_n\n",
            "  m.04g4s8k -[location.imports_exports_by_industry.industry (0.3083)]-> m.03qtf10\n",
            "  m.04g4s8w -[location.imports_exports_by_industry.industry (0.3083)]-> m.03q9wp2\n",
            "  m.04g4s8w -[location.imports_exports_by_industry.industry (0.3083)]-> m.015smg\n",
            "  m.04g4s8w -[location.imports_exports_by_industry.industry (0.3083)]-> m.03q9wp2\n",
            "  m.04g4s8w -[location.imports_exports_by_industry.industry (0.3083)]-> m.03q9wp2\n",
            "  m.04g4s8w -[location.imports_exports_by_industry.industry (0.3083)]-> m.015smg\n",
            "  m.04g4s8w -[location.imports_exports_by_industry.industry (0.3083)]-> m.03q9wp2\n",
            "  m.04g4s90 -[location.imports_exports_by_industry.industry (0.3083)]-> m.03qtf10\n",
            "  m.04g4s90 -[location.imports_exports_by_industry.industry (0.3083)]-> m.015smg\n",
            "  m.04g4s90 -[location.imports_exports_by_industry.industry (0.3083)]-> m.03q9wp2\n",
            "  m.04g4s8q -[location.imports_exports_by_industry.industry (0.3083)]-> m.03qtf10\n",
            "  m.04g4s8q -[location.imports_exports_by_industry.industry (0.3083)]-> m.03qtd_n\n",
            "  m.04g4s8q -[location.imports_exports_by_industry.industry (0.3083)]-> m.03q9wp2\n",
            "  m.03q1lwl -[location.religion_percentage.religion (0.2777)]-> m.01lp8\n",
            "  m.03q1lwl -[location.religion_percentage.religion (0.2777)]-> m.03j6c\n",
            "  m.03q1lwl -[location.religion_percentage.religion (0.2777)]-> m.03_gx\n",
            "  m.03q1lvq -[location.religion_percentage.religion (0.2777)]-> m.01lp8\n",
            "  m.03q1lvq -[location.religion_percentage.religion (0.2777)]-> m.092bf5\n",
            "  m.03q1lvq -[location.religion_percentage.religion (0.2777)]-> m.03_gx\n",
            "  Top 3 beams:\n",
            "    1. Path: m.09c7w0 -> m.04g4s8k -> m.03qtd_n... (score: 0.3085)\n",
            "    2. Path: m.09c7w0 -> m.04g4s8k -> m.03qtd_n... (score: 0.3085)\n",
            "    3. Path: m.09c7w0 -> m.04g4s8k -> m.03qtf10... (score: 0.3085)\n",
            "\n",
            "Depth 3:\n",
            "  Node m.03qtd_n has no outgoing edges, keeping as final\n",
            "  Node m.03qtd_n has no outgoing edges, keeping as final\n",
            "  Node m.03qtf10 has no outgoing edges, keeping as final\n",
            "  Node m.03q9wp2 has no outgoing edges, keeping as final\n",
            "  Node m.015smg has no outgoing edges, keeping as final\n",
            "  Node m.03qtf10 has no outgoing edges, keeping as final\n",
            "  Node m.015smg has no outgoing edges, keeping as final\n",
            "  Node m.03q9wp2 has no outgoing edges, keeping as final\n",
            "  Node m.03qtd_n has no outgoing edges, keeping as final\n",
            "  Node m.015smg has no outgoing edges, keeping as final\n",
            "  No more valid paths, stopping search\n",
            "\n",
            "Final predictions (4):\n",
            "  - m.03qtd_n\n",
            "  - m.03q9wp2\n",
            "  - m.03qtf10\n",
            "  - m.015smg\n",
            "\n",
            "Predictions (4):\n",
            "  - m.03qtd_n\n",
            "  - m.03q9wp2\n",
            "  - m.03qtf10\n",
            "  - m.015smg\n",
            "\n",
            "Results:\n",
            "  Correct predictions: 4\n",
            "  Missed answers: 0\n",
            "  Extra predictions: 0\n",
            "  F1 score: 1.0000\n",
            "\n",
            "================================================================================\n",
            "DEBUGGING QUERY #3: what time is right now in texas\n",
            "Start node: m.07b_l\n",
            "Number of expected answers: 2\n",
            "Sample expected answers:\n",
            "  - m.02fqwt\n",
            "  - m.02hczc\n",
            "\n",
            "Starting beam search from node m.07b_l for query: what time is right now in texas\n",
            "\n",
            "Depth 1:\n",
            "  m.07b_l -[location.location.time_zones (0.3505)]-> m.02hczc\n",
            "  m.07b_l -[location.us_state.capital (0.2191)]-> m.0vzm\n",
            "  m.07b_l -[base.aareas.schema.administrative_area.administrative_parent (0.2761)]-> m.09c7w0\n",
            "  Top 3 beams:\n",
            "    1. Path: m.07b_l -> m.02hczc... (score: 0.3505)\n",
            "    2. Path: m.07b_l -> m.02hczc... (score: 0.3505)\n",
            "    3. Path: m.07b_l -> m.02fqwt... (score: 0.3505)\n",
            "\n",
            "Depth 2:\n",
            "  m.02hczc -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.09c7w0\n",
            "  m.02hczc -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.059g4\n",
            "  m.02hczc -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.09c7w0\n",
            "  m.02hczc -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.059g4\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.021czc\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.01_d4\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.03v0t\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.021czc\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.01_d4\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.03v0t\n",
            "  m.09c7w0 -[location.statistical_region.religions (0.1964)]-> m.03q1lwl\n",
            "  m.09c7w0 -[location.statistical_region.religions (0.1964)]-> m.03q1lvq\n",
            "  m.09c7w0 -[location.location.contains (0.1778)]-> m.0vzm\n",
            "  m.09c7w0 -[location.statistical_region.religions (0.1964)]-> m.03q1lwl\n",
            "  m.09c7w0 -[location.statistical_region.religions (0.1964)]-> m.03q1lvq\n",
            "  m.09c7w0 -[location.location.contains (0.1778)]-> m.0vzm\n",
            "  m.09c7w0 -[location.statistical_region.religions (0.1964)]-> m.03q1lwl\n",
            "  m.09c7w0 -[location.statistical_region.religions (0.1964)]-> m.03q1lvq\n",
            "  m.09c7w0 -[location.location.contains (0.1778)]-> m.0vzm\n",
            "  m.0vzm -[location.location.time_zones (0.3505)]-> m.02fqwt\n",
            "  m.0vzm -[location.location.containedby (0.2098)]-> m.09c7w0\n",
            "  m.0vzm -[base.biblioness.bibs_location.country (0.3039)]-> m.09c7w0\n",
            "  m.0vzm -[location.location.time_zones (0.3505)]-> m.02fqwt\n",
            "  m.0vzm -[location.location.containedby (0.2098)]-> m.09c7w0\n",
            "  m.0vzm -[base.biblioness.bibs_location.country (0.3039)]-> m.09c7w0\n",
            "  m.0vzm -[location.location.time_zones (0.3505)]-> m.02fqwt\n",
            "  m.0vzm -[location.location.containedby (0.2098)]-> m.09c7w0\n",
            "  m.0vzm -[base.biblioness.bibs_location.country (0.3039)]-> m.09c7w0\n",
            "  Top 3 beams:\n",
            "    1. Path: m.07b_l -> m.09c7w0 -> m.027wj2_... (score: 0.3272)\n",
            "    2. Path: m.07b_l -> m.09c7w0 -> m.02fqwt... (score: 0.3272)\n",
            "    3. Path: m.07b_l -> m.09c7w0 -> m.042g7t... (score: 0.3272)\n",
            "\n",
            "Depth 3:\n",
            "  No valid expansions from m.027wj2_, keeping as final\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.021czc\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.01_d4\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.03v0t\n",
            "  m.042g7t -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.059g4\n",
            "  m.02lcqs -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.059g4\n",
            "  m.02hczc -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.059g4\n",
            "  m.02lcrv -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.059g4\n",
            "  No valid expansions from m.027wjl3, keeping as final\n",
            "  m.02lctm -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.059g4\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.021czc\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.01_d4\n",
            "  m.02fqwt -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.03v0t\n",
            "  m.02lcrv -[time.time_zone.locations_in_this_time_zone (0.2913)]-> m.059g4\n",
            "  Top 3 beams:\n",
            "    1. Path: m.09c7w0 -> m.02fqwt -> m.021czc... (score: 0.3153)\n",
            "    2. Path: m.09c7w0 -> m.02fqwt -> m.01_d4... (score: 0.3153)\n",
            "    3. Path: m.09c7w0 -> m.02fqwt -> m.03v0t... (score: 0.3153)\n",
            "\n",
            "Depth 4:\n",
            "  m.021czc -[location.location.containedby (0.2098)]-> m.01_d4\n",
            "  m.021czc -[location.location.containedby (0.2098)]-> m.03v0t\n",
            "  m.01_d4 -[location.citytown.postal_codes (0.2645)]-> m.07nqmhw\n",
            "  m.01_d4 -[location.citytown.postal_codes (0.2645)]-> m.07nqmhf\n",
            "  m.01_d4 -[location.citytown.postal_codes (0.2645)]-> m.01_6pxw\n",
            "  m.03v0t -[location.location.contains (0.1778)]-> m.02jw0z\n",
            "  m.03v0t -[location.location.contains (0.1778)]-> m.01_d4\n",
            "  m.03v0t -[location.location.contains (0.1778)]-> m.01_6pxw\n",
            "  m.059g4 -[location.location.time_zones (0.3505)]-> m.042g7t\n",
            "  m.059g4 -[location.location.time_zones (0.3505)]-> m.02lcqs\n",
            "  m.059g4 -[location.location.time_zones (0.3505)]-> m.02hczc\n",
            "  m.01dlzc -[location.location.containedby (0.2098)]-> m.01_d4\n",
            "  m.01dlzc -[location.location.containedby (0.2098)]-> m.03v0t\n",
            "  No valid expansions from m.0vzm, keeping as final\n",
            "  m.059g4 -[location.location.time_zones (0.3505)]-> m.02lcqs\n",
            "  m.059g4 -[location.location.time_zones (0.3505)]-> m.02hczc\n",
            "  m.059g4 -[location.location.events (0.2229)]-> m.09r3f\n",
            "  m.059g4 -[location.location.time_zones (0.3505)]-> m.042g7t\n",
            "  m.059g4 -[location.location.time_zones (0.3505)]-> m.02hczc\n",
            "  m.059g4 -[location.location.events (0.2229)]-> m.09r3f\n",
            "  m.059g4 -[location.location.time_zones (0.3505)]-> m.042g7t\n",
            "  m.059g4 -[location.location.time_zones (0.3505)]-> m.02lcqs\n",
            "  m.059g4 -[location.location.events (0.2229)]-> m.09r3f\n",
            "  m.059g4 -[location.location.time_zones (0.3505)]-> m.042g7t\n",
            "  m.059g4 -[location.location.time_zones (0.3505)]-> m.02lcqs\n",
            "  m.059g4 -[location.location.time_zones (0.3505)]-> m.02hczc\n",
            "  Top 3 beams:\n",
            "    1. Path: m.02fqwt -> m.059g4 -> m.042g7t... (score: 0.3241)\n",
            "    2. Path: m.02fqwt -> m.059g4 -> m.02lcqs... (score: 0.3241)\n",
            "    3. Path: m.02fqwt -> m.059g4 -> m.02hczc... (score: 0.3241)\n",
            "\n",
            "Final predictions (9):\n",
            "  - m.02hcv8\n",
            "  - m.02lcrv\n",
            "  - m.027wj2_\n",
            "  - m.0vzm\n",
            "  - m.02hczc\n",
            "\n",
            "Predictions (9):\n",
            "  - m.02hcv8\n",
            "  - m.02lcrv\n",
            "  - m.027wj2_\n",
            "  - m.0vzm\n",
            "  - m.02hczc\n",
            "\n",
            "Results:\n",
            "  Correct predictions: 1\n",
            "  Missed answers: 1\n",
            "  Extra predictions: 8\n",
            "  F1 score: 0.1818\n",
            "\n",
            "Per-query performance:\n",
            "   ID   F1 Score   Gold Count   Pred Count\n",
            "---------------------------------------------\n",
            "    1     0.6000            9           11\n",
            "    2     1.0000            4            4\n",
            "    3     0.1818            2            9\n",
            "    4     0.4615           30            9\n",
            "    5     0.7143            7            7\n",
            "    6     0.0000            3           10\n",
            "    7     0.2857            1            6\n",
            "    8     0.3529            3           14\n",
            "    9     0.0000            1           10\n",
            "   10     0.4444            5            4\n",
            "\n",
            "F1 Score statistics:\n",
            "  Mean: 0.2335\n",
            "  Median: 0.1818\n",
            "  Min: 0.0000\n",
            "  Max: 1.0000\n",
            "  Queries with F1 > 0: 40/56\n",
            "\n",
            "Overall F1 score: 0.1998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance Summary\n",
        "\n",
        "A beam‐search QA pipeline was evaluated on a Freebase‐derived subgraph (286 nodes, 1,761 edges) against 56 natural‐language queries. Across all questions, the system achieved an overall F₁ score of 0.20 (mean F₁ = 0.2335, median F₁ = 0.1818), with individual query scores ranging from 0.00 to 1.00. Forty out of 56 queries produced at least one correct answer, though several queries returned either too many spurious nodes or missed all positives entirely.\n",
        "\n",
        "Interpretation\n",
        "\n",
        "High variability in per‐query performance indicates that the current similarity threshold and path‐scoring heuristic capture some relation types very well (e.g., question #2 achieved F₁ = 1.00) but struggle on others. Low‐scoring queries often suffer from overly permissive expansions that introduce false positives, while an overly strict threshold can prematurely terminate promising paths."
      ],
      "metadata": {
        "id": "frY49VHt7pW6"
      }
    }
  ]
}