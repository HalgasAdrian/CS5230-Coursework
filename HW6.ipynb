{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPeeUWlSkvDudCfhriztj2/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1: Recommender System using Collaborative Filtering**\n",
        "\n",
        "Implement a Movie Recommendation System and run it on the Movie Lens Dataset (Train vs Test). Mesure performance on test set using RMSE\n",
        "\n",
        "\n",
        "\n",
        "*   First you are required to compute first a user-user similarity based on ratings and movies in common\n",
        "*   Second, make rating predictions on the test set following the KNN idea: a prediction (user, movie) is the weighted average of other users' rating for the movie, weighted by user-similarity to the given user."
      ],
      "metadata": {
        "id": "anhmU0COXwct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSRJYASjVOzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb14466-edb5-40d8-c469-2fdc511783d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE (user-user CF, k=5): 2.4288\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load Movielens 100k data\n",
        "# Download and extract the dataset, then point to the u.data file\n",
        "with zipfile.ZipFile('ml-100k.zip','r') as z:\n",
        "    with z.open('ml-100k/u.data') as f:\n",
        "        ratings = pd.read_csv(\n",
        "            f,\n",
        "            sep='\\t',\n",
        "            names=['user_id','item_id','rating','timestamp'],\n",
        "            usecols=['user_id','item_id','rating']\n",
        "        )\n",
        "\n",
        "# 2. Train/test split\n",
        "train_df, test_df = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Build user-item rating matrix from train set\n",
        "ratings_matrix = train_df.pivot_table(\n",
        "    index='user_id', columns='item_id', values='rating'\n",
        ")\n",
        "\n",
        "# 4. Compute user-user similarity (Pearson)\n",
        "# We use pandas' corr on transposed matrix: correlations over items axis\n",
        "sim_matrix = ratings_matrix.T.corr(method='pearson', min_periods=1)\n",
        "\n",
        "# 5. Prediction function using KNN\n",
        "\n",
        "def predict_rating(user_id, item_id, ratings_mat, sim_mat, k=5):\n",
        "    if item_id not in ratings_mat.columns:\n",
        "        # Movie not seen in training\n",
        "        return ratings_mat.stack().mean()\n",
        "\n",
        "    # similarities for target user to all others\n",
        "    sims = sim_mat[user_id].drop(index=user_id).dropna()\n",
        "    # ratings of other users for this item\n",
        "    item_ratings = ratings_mat[item_id].dropna()\n",
        "\n",
        "    # intersect neighbors\n",
        "    common_users = sims.index.intersection(item_ratings.index)\n",
        "    if len(common_users) == 0:\n",
        "        return ratings_mat.stack().mean()\n",
        "\n",
        "    # select top-k similar users\n",
        "    top_k = sims.loc[common_users].abs().nlargest(k).index\n",
        "    top_sims = sims.loc[top_k]\n",
        "    top_ratings = item_ratings.loc[top_k]\n",
        "\n",
        "    # weighted average\n",
        "    num = (top_sims * top_ratings).sum()\n",
        "    den = top_sims.abs().sum()\n",
        "    if den == 0:\n",
        "        return ratings_mat.stack().mean()\n",
        "    return num / den\n",
        "\n",
        "# 6. Predict for test set and evaluate RMSE\n",
        "preds = []\n",
        "truths = []\n",
        "for _, row in test_df.iterrows():\n",
        "    u, i, r = row['user_id'], row['item_id'], row['rating']\n",
        "    pred = predict_rating(u, i, ratings_matrix, sim_matrix, k=5)\n",
        "    preds.append(pred)\n",
        "    truths.append(r)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(truths, preds))\n",
        "print(f\"Test RMSE (user-user CF, k=5): {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3A: Social Community Detection**\n",
        "\n",
        "Implement edge-removal community detection algorithm on the Flicker Graph. Use the betweeness idea on edges and the Girvan–Newman Algorithm. The original dataset graph has more than 5M edges; in DM_resources there are 4 different sub-sampled graphs with edge counts from 2K to 600K; you can use these if the original is too big.\n",
        "You should use a library to support graph operations (edges, vertices, paths, degrees, etc). We used igraph in python which also have builtin community detection algorithms (not allowed); these are useful as a way to evaluate communities you obtain"
      ],
      "metadata": {
        "id": "z6mZswjbYnYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install igraph’s C core\n",
        "!apt-get update -qq\n",
        "!apt-get install -y libigraph0-dev\n",
        "\n",
        "# install the Python bindings\n",
        "!pip install python-igraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ut6dJtQYumf",
        "outputId": "fbeeed0b-3810-4b4c-a388-643faf448147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package libigraph0-dev is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "\n",
            "E: Package 'libigraph0-dev' has no installation candidate\n",
            "Requirement already satisfied: python-igraph in /usr/local/lib/python3.11/dist-packages (0.11.8)\n",
            "Requirement already satisfied: igraph==0.11.8 in /usr/local/lib/python3.11/dist-packages (from python-igraph) (0.11.8)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from igraph==0.11.8->python-igraph) (1.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gJYxrhAWk8xK",
        "outputId": "b4eddbe8-1115-4334-c1a1-a0fb447f8f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "This module defines load_graph() and girvan_newman(); import and call them interactively.",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m This module defines load_graph() and girvan_newman(); import and call them interactively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3B: Social Community Detection**\n",
        "\n",
        "Implement the modularity detection algorithm on this artificial graph (adj matrix written in sparse format : each row is an edge [node_id, node_id, 1]). You will need to compute the modularity matrix B and its highest-val eigenvector V1. The split vector S (+1 / -1) aligns by sign with V1; follow this paper. Partition the graph in two parts K=2).\n",
        "Optional: Partition the graph in more than 2 parts, try to figure out what is a \"natural\" K here."
      ],
      "metadata": {
        "id": "KkdSlJf9YvCX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OOsFiP3cY1l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 4: Knowledge Base Question Answering**\n",
        "\n",
        "Given is knowledge graph with entities and relations, questions with starting entity and answers, and their word embedding . For each question, navigate the graph from the start entity outwards until you find appropriate answer entities.\n",
        "Utils functions (similarity, load_graphs) are given, but you can choose not to use them. This python file contains the helper functions for this homework, the only update needed to use this file is to fill in the file paths.\n",
        "\n",
        "- The number of correct answers varies (could be 1, could many), use F1 to compare your answers with the given correct answers\n",
        "- Utils functions (similarity, load_graphs) are given, but you can choose not to use them.\n",
        "- Answers are given to be used for evaluation only, DO NOT USE ANSWERS IN YOUR GRAPH TRAVERSAL.\n",
        "Your strategy should be a graph traversal augmented with scoring of paths; you might discard paths not promising along the way. This is similar to a focused crawl strategy. You will take a query (question) that you are trying to answer, it will have a starting entity. Begin your traversal at that starting entity, and look at all adjacent edges. Use get_rel_score_word2vecbase to get a similarity score for each edge, and traverse the edges that are promising. This part is up to you, you can cut off scores below a certain threshold, or take only the top percentage, or weight it based on the average.\n",
        "\n",
        "There are many valid strategies. You will continue to traverse a path, until the score starts to decrease, or you notice the similarity score drops significantly (compared to the previous edges). Overall try a few different approaches, and choose one that gives you the best overall F1 score."
      ],
      "metadata": {
        "id": "gR94tZjQY20p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install numpy==1.26.4\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "JEa5EdNzdq_g",
        "outputId": "9912b192-f6b5-44a1-f6bb-742c10febd55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "from gensim import models\n",
        "from nltk import word_tokenize\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Function to get the cosine similarity between a relation and query\n",
        "# Note: Update this string with the path to the file\n",
        "word2vec_model = models.Word2Vec.load('../data/Graphs/word2vec_train_dev.dat')\n",
        "\n",
        "\n",
        "def get_rel_score_word2vecbase(rel: str, query: str) -> float:\n",
        "    \"\"\"\n",
        "    Get score for query and relation. Used to inform exploration of knowledge graph.\n",
        "\n",
        "    :param rel: relation, or edge in knowledge graph\n",
        "    :param query: query, question to answer\n",
        "    :return: float score similarity between question and relation\n",
        "    \"\"\"\n",
        "    # Relation not in embedding vocabulary\n",
        "    if rel not in word2vec_model.wv:\n",
        "        return 0.0\n",
        "    # Relation must start with ns:\n",
        "    rel = 'ns:' + rel if not rel[:3] == 'ns:' else rel\n",
        "\n",
        "    words = word_tokenize(query.lower())\n",
        "    w_embs = []\n",
        "    for w in words:\n",
        "        if w in word2vec_model.wv:\n",
        "            w_embs.append(word2vec_model.wv[w])\n",
        "    return np.mean(cosine_similarity(w_embs, [word2vec_model.wv[rel]]))\n",
        "\n",
        "\n",
        "def load_node_label_lookup(filepath: str) -> dict:\n",
        "    \"\"\"\n",
        "\n",
        "    Load the lookup dictionary for nodes from the provided json file.\n",
        "\n",
        "    Args:\n",
        "        filepath: Path to the json file containing the lookup dictionary.\n",
        "\n",
        "    Returns: Dictionary of node ids to text description of node.\n",
        "\n",
        "    \"\"\"\n",
        "    with open(filepath, 'rb') as fp:\n",
        "        return json.load(fp)\n",
        "\n",
        "\n",
        "def load_query_df(filepath: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "\n",
        "    Load a simplified dataframe of queries. Generated from the original queries nested dictionary, this simplified\n",
        "    version contains all necessary information for performing the graph traversal testing without all the extra\n",
        "    information and difficult formatting. Simply loop through this dataframe row by row, start at the start node with\n",
        "    the query for that row, and the expected answers are given in that same row.\n",
        "\n",
        "    Args:\n",
        "        filepath: Path to the provided parquet file\n",
        "\n",
        "    Returns: Dataframe of queries to perform on the graph.\n",
        "\n",
        "    \"\"\"\n",
        "    return pd.read_parquet(filepath)\n",
        "\n",
        "\n",
        "# Function to load the graph from file\n",
        "def load_graph() -> dict:\n",
        "    \"\"\"\n",
        "\n",
        "    Load the graph from the given file.\n",
        "\n",
        "    Returns: Graph, in form of node_id key, and nested list value. Nested list is adjacency list, with each list\n",
        "    containing the relation, and destination node_id.\n",
        "\n",
        "    \"\"\"\n",
        "    # Preparing the graph\n",
        "    graph = defaultdict(list)\n",
        "    for line in open('../data/Graphs/graph.txt'):\n",
        "        line = eval(line[:-1])\n",
        "        graph[line[0]].append([line[1], line[2]])\n",
        "    return graph\n",
        "\n",
        "\n",
        "# Function to load the queries from file\n",
        "# Preparing the queries\n",
        "def load_queries() -> list:\n",
        "    \"\"\"\n",
        "\n",
        "    Load the original queries file. This format can be extremely confusing, for a simplified format use load_query_df.\n",
        "\n",
        "    Returns: Nested list, with index, node_id, relation types for answers, text description of start node, and dict of\n",
        "    answers.\n",
        "\n",
        "    \"\"\"\n",
        "    queries = []\n",
        "    for line in open('../data/Graphs/annotations.txt'):\n",
        "        line = eval(line[:-1])\n",
        "        queries.append(line)\n",
        "    return queries"
      ],
      "metadata": {
        "id": "bMJvXy-XY-eA",
        "outputId": "3ae8c775-bf06-447b-8b68-d5f6314299cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../data/Graphs/word2vec_train_dev.dat'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-35f6ccbb68b5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Function to get the cosine similarity between a relation and query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Note: Update this string with the path to the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mword2vec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/Graphs/word2vec_train_dev.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1951\u001b[0m         \"\"\"\n\u001b[1;32m   1952\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1954\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m                 \u001b[0mrethrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_lifecycle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \"\"\"\n\u001b[0;32m-> 1459\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# needed because loading from S3 doesn't support readline()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/Graphs/word2vec_train_dev.dat'"
          ]
        }
      ]
    }
  ]
}