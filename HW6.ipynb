{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN911gxoNET7AHp+OcUfv5d"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1: Recommender System using Collaborative Filtering**\n",
        "\n",
        "Implement a Movie Recommendation System and run it on the Movie Lens Dataset (Train vs Test). Mesure performance on test set using RMSE\n",
        "\n",
        "\n",
        "\n",
        "*   First you are required to compute first a user-user similarity based on ratings and movies in common\n",
        "*   Second, make rating predictions on the test set following the KNN idea: a prediction (user, movie) is the weighted average of other users' rating for the movie, weighted by user-similarity to the given user."
      ],
      "metadata": {
        "id": "anhmU0COXwct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZSRJYASjVOzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb14466-edb5-40d8-c469-2fdc511783d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE (user-user CF, k=5): 2.4288\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load Movielens 100k data\n",
        "# Download and extract the dataset, then point to the u.data file\n",
        "with zipfile.ZipFile('ml-100k.zip','r') as z:\n",
        "    with z.open('ml-100k/u.data') as f:\n",
        "        ratings = pd.read_csv(\n",
        "            f,\n",
        "            sep='\\t',\n",
        "            names=['user_id','item_id','rating','timestamp'],\n",
        "            usecols=['user_id','item_id','rating']\n",
        "        )\n",
        "\n",
        "# 2. Train/test split\n",
        "train_df, test_df = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Build user-item rating matrix from train set\n",
        "ratings_matrix = train_df.pivot_table(\n",
        "    index='user_id', columns='item_id', values='rating'\n",
        ")\n",
        "\n",
        "# 4. Compute user-user similarity (Pearson)\n",
        "# We use pandas' corr on transposed matrix: correlations over items axis\n",
        "sim_matrix = ratings_matrix.T.corr(method='pearson', min_periods=1)\n",
        "\n",
        "# 5. Prediction function using KNN\n",
        "\n",
        "def predict_rating(user_id, item_id, ratings_mat, sim_mat, k=5):\n",
        "    if item_id not in ratings_mat.columns:\n",
        "        # Movie not seen in training\n",
        "        return ratings_mat.stack().mean()\n",
        "\n",
        "    # similarities for target user to all others\n",
        "    sims = sim_mat[user_id].drop(index=user_id).dropna()\n",
        "    # ratings of other users for this item\n",
        "    item_ratings = ratings_mat[item_id].dropna()\n",
        "\n",
        "    # intersect neighbors\n",
        "    common_users = sims.index.intersection(item_ratings.index)\n",
        "    if len(common_users) == 0:\n",
        "        return ratings_mat.stack().mean()\n",
        "\n",
        "    # select top-k similar users\n",
        "    top_k = sims.loc[common_users].abs().nlargest(k).index\n",
        "    top_sims = sims.loc[top_k]\n",
        "    top_ratings = item_ratings.loc[top_k]\n",
        "\n",
        "    # weighted average\n",
        "    num = (top_sims * top_ratings).sum()\n",
        "    den = top_sims.abs().sum()\n",
        "    if den == 0:\n",
        "        return ratings_mat.stack().mean()\n",
        "    return num / den\n",
        "\n",
        "# 6. Predict for test set and evaluate RMSE\n",
        "preds = []\n",
        "truths = []\n",
        "for _, row in test_df.iterrows():\n",
        "    u, i, r = row['user_id'], row['item_id'], row['rating']\n",
        "    pred = predict_rating(u, i, ratings_matrix, sim_matrix, k=5)\n",
        "    preds.append(pred)\n",
        "    truths.append(r)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(truths, preds))\n",
        "print(f\"Test RMSE (user-user CF, k=5): {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3A: Social Community Detection**\n",
        "\n",
        "Implement edge-removal community detection algorithm on the Flicker Graph. Use the betweeness idea on edges and the Girvan–Newman Algorithm. The original dataset graph has more than 5M edges; in DM_resources there are 4 different sub-sampled graphs with edge counts from 2K to 600K; you can use these if the original is too big.\n",
        "You should use a library to support graph operations (edges, vertices, paths, degrees, etc). We used igraph in python which also have builtin community detection algorithms (not allowed); these are useful as a way to evaluate communities you obtain"
      ],
      "metadata": {
        "id": "z6mZswjbYnYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install igraph’s C core\n",
        "!apt-get update -qq\n",
        "!apt-get install -y libigraph0-dev\n",
        "\n",
        "# install the Python bindings\n",
        "!pip install python-igraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ut6dJtQYumf",
        "outputId": "fbeeed0b-3810-4b4c-a388-643faf448147"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package libigraph0-dev is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "\n",
            "E: Package 'libigraph0-dev' has no installation candidate\n",
            "Requirement already satisfied: python-igraph in /usr/local/lib/python3.11/dist-packages (0.11.8)\n",
            "Requirement already satisfied: igraph==0.11.8 in /usr/local/lib/python3.11/dist-packages (from python-igraph) (0.11.8)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from igraph==0.11.8->python-igraph) (1.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gJYxrhAWk8xK",
        "outputId": "b4eddbe8-1115-4334-c1a1-a0fb447f8f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "This module defines load_graph() and girvan_newman(); import and call them interactively.",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m This module defines load_graph() and girvan_newman(); import and call them interactively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3B: Social Community Detection**\n",
        "\n",
        "Implement the modularity detection algorithm on this artificial graph (adj matrix written in sparse format : each row is an edge [node_id, node_id, 1]). You will need to compute the modularity matrix B and its highest-val eigenvector V1. The split vector S (+1 / -1) aligns by sign with V1; follow this paper. Partition the graph in two parts K=2).\n",
        "Optional: Partition the graph in more than 2 parts, try to figure out what is a \"natural\" K here."
      ],
      "metadata": {
        "id": "KkdSlJf9YvCX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OOsFiP3cY1l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 4: Knowledge Base Question Answering**\n",
        "\n",
        "Given is knowledge graph with entities and relations, questions with starting entity and answers, and their word embedding . For each question, navigate the graph from the start entity outwards until you find appropriate answer entities.\n",
        "Utils functions (similarity, load_graphs) are given, but you can choose not to use them. This python file contains the helper functions for this homework, the only update needed to use this file is to fill in the file paths.\n",
        "\n",
        "- The number of correct answers varies (could be 1, could many), use F1 to compare your answers with the given correct answers\n",
        "- Utils functions (similarity, load_graphs) are given, but you can choose not to use them.\n",
        "- Answers are given to be used for evaluation only, DO NOT USE ANSWERS IN YOUR GRAPH TRAVERSAL.\n",
        "Your strategy should be a graph traversal augmented with scoring of paths; you might discard paths not promising along the way. This is similar to a focused crawl strategy. You will take a query (question) that you are trying to answer, it will have a starting entity. Begin your traversal at that starting entity, and look at all adjacent edges. Use get_rel_score_word2vecbase to get a similarity score for each edge, and traverse the edges that are promising. This part is up to you, you can cut off scores below a certain threshold, or take only the top percentage, or weight it based on the average.\n",
        "\n",
        "There are many valid strategies. You will continue to traverse a path, until the score starts to decrease, or you notice the similarity score drops significantly (compared to the previous edges). Overall try a few different approaches, and choose one that gives you the best overall F1 score."
      ],
      "metadata": {
        "id": "gR94tZjQY20p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bMJvXy-XY-eA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}